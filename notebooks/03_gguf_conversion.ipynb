{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1iWGXZ3TIgoM0BuZy5VkIiS12IQm3-bIT","timestamp":1751642200595},{"file_id":"1gu3WoGPqFgbd8hi2HYtdQsnrjyK8UgMA","timestamp":1751640758594}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"589bbddf090e44a68d1489d87d566dfe":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":[],"layout":"IPY_MODEL_4a714d3a2d5342b58181bde2b59cadb8"}},"ab82c17455884725a9cdab8db298ef96":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fc68682e506c497faf26a4f1bf08a295","placeholder":"​","style":"IPY_MODEL_162977cb0e86462dab260f4a4aa8f25e","value":"<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"}},"a24ee43e31e747efa49225db96764c6f":{"model_module":"@jupyter-widgets/controls","model_name":"PasswordModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PasswordModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PasswordView","continuous_update":true,"description":"Token:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_0e509d085e694952a6c891213faeb631","placeholder":"​","style":"IPY_MODEL_075612931faa47b1ab3a8a5a6c679b6e","value":""}},"6181905329ab44c3b19e1d8390f364af":{"model_module":"@jupyter-widgets/controls","model_name":"CheckboxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"CheckboxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"CheckboxView","description":"Add token as git credential?","description_tooltip":null,"disabled":false,"indent":true,"layout":"IPY_MODEL_67de91bc52514fec9c31d7e8b151cbfe","style":"IPY_MODEL_78142030b734446cb488096e4055d048","value":true}},"dc551824977248258e8e60f6abccbef3":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Login","disabled":false,"icon":"","layout":"IPY_MODEL_b9a4d1d3a05a42d4924efc785247bd9a","style":"IPY_MODEL_8db16a9d276d456ebcd626480fb8b134","tooltip":""}},"a0ac5feecf9247f388abcad004231494":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_52fb75ef1e0443be875f83a4e2f3206c","placeholder":"​","style":"IPY_MODEL_ded023b4b22e4fcdbf50b987b602931b","value":"\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"}},"4a714d3a2d5342b58181bde2b59cadb8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"fc68682e506c497faf26a4f1bf08a295":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"162977cb0e86462dab260f4a4aa8f25e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0e509d085e694952a6c891213faeb631":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"075612931faa47b1ab3a8a5a6c679b6e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"67de91bc52514fec9c31d7e8b151cbfe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"78142030b734446cb488096e4055d048":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b9a4d1d3a05a42d4924efc785247bd9a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8db16a9d276d456ebcd626480fb8b134":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"52fb75ef1e0443be875f83a4e2f3206c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ded023b4b22e4fcdbf50b987b602931b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6882db340a794962aae9a141ad8c177d":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d01f8b682d2e419580fd186fb2e8ee46","placeholder":"​","style":"IPY_MODEL_8b77ab6cc3d14070a6ea0b599923d828","value":"Connecting..."}},"d01f8b682d2e419580fd186fb2e8ee46":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b77ab6cc3d14070a6ea0b599923d828":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"01640ce31be34053a08820ed24dd3c3b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_78f06655bc4c425e8ff3555af7bdfe80","IPY_MODEL_9724992f57904465b001e2b20874eafb","IPY_MODEL_3f9c4198d05545d2860058d6358b2c8e"],"layout":"IPY_MODEL_355dafac28db419fb33467a7bfd119b1"}},"78f06655bc4c425e8ff3555af7bdfe80":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0967dad5e68748e1b3d1cf5ff0e09f39","placeholder":"​","style":"IPY_MODEL_194ab9ed3612455baaa97bbc4b4a7fe8","value":"Fetching 13 files: 100%"}},"9724992f57904465b001e2b20874eafb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d20b3d425e89418b8ba4173da9e6b351","max":13,"min":0,"orientation":"horizontal","style":"IPY_MODEL_35acae95a6584f44b7b8362d7a345a82","value":13}},"3f9c4198d05545d2860058d6358b2c8e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a600618f6c7048b0b28e2d890e9ebbed","placeholder":"​","style":"IPY_MODEL_c4e616b1d17b407986809eae73e4b0f2","value":" 13/13 [00:08&lt;00:00,  1.04it/s]"}},"355dafac28db419fb33467a7bfd119b1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0967dad5e68748e1b3d1cf5ff0e09f39":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"194ab9ed3612455baaa97bbc4b4a7fe8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d20b3d425e89418b8ba4173da9e6b351":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35acae95a6584f44b7b8362d7a345a82":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a600618f6c7048b0b28e2d890e9ebbed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c4e616b1d17b407986809eae73e4b0f2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b632f12aa4f143d48a5a429c90ea88a9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6f6dc6da3e2c4fc5bfa6a6e2d071b4eb","IPY_MODEL_efeffde1e060494794a8f930c47e520f","IPY_MODEL_36a0b3f0e7344c76b1e26ecef9cb7a7b"],"layout":"IPY_MODEL_e96581c1d8304bc2a61637b3740b6f10"}},"6f6dc6da3e2c4fc5bfa6a6e2d071b4eb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_685e9159a49d4bebaf4bbcd220839623","placeholder":"​","style":"IPY_MODEL_86df176c06d54127b13e6f23d49f0d31","value":"adapter_model.safetensors: 100%"}},"efeffde1e060494794a8f930c47e520f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cf158d9cc40c4393b58c011b4fd24e21","max":40,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2eb97bbd40514969a98c64c961f45433","value":40}},"36a0b3f0e7344c76b1e26ecef9cb7a7b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a9f91d7695f04e3b84e5a80cbd9b966f","placeholder":"​","style":"IPY_MODEL_361a49287fa34c648931f75147173249","value":" 40.0/40.0 [00:00&lt;00:00, 468B/s]"}},"e96581c1d8304bc2a61637b3740b6f10":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"685e9159a49d4bebaf4bbcd220839623":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86df176c06d54127b13e6f23d49f0d31":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cf158d9cc40c4393b58c011b4fd24e21":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2eb97bbd40514969a98c64c961f45433":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a9f91d7695f04e3b84e5a80cbd9b966f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"361a49287fa34c648931f75147173249":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1519d7252c3243499aa65de7db9682db":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cf7e87353fcd488e8b26c28c27b3fcdf","IPY_MODEL_2431a1f76ba149aba4c8fd3ab8cb1fa4","IPY_MODEL_0fc6a0c7cafa426382affb03897e8d8e"],"layout":"IPY_MODEL_ee36d175ca9847e9b70f310edcf393b2"}},"cf7e87353fcd488e8b26c28c27b3fcdf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0cdbc154674542bc9744fd8d664717d7","placeholder":"​","style":"IPY_MODEL_0cf3e4ffb75e4e7aabdaa68e401b0a8c","value":".gitattributes: "}},"2431a1f76ba149aba4c8fd3ab8cb1fa4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fef88ebc9d96483d89abf198226b6b0f","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_026ffc4ef9114a3fba1c63a8cbe887d7","value":1}},"0fc6a0c7cafa426382affb03897e8d8e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9f259f93ab12482d828ba001c07d8328","placeholder":"​","style":"IPY_MODEL_1ba2d4d6d0844e76ba9287ecee700dc3","value":" 1.52k/? [00:00&lt;00:00, 11.4kB/s]"}},"ee36d175ca9847e9b70f310edcf393b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0cdbc154674542bc9744fd8d664717d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0cf3e4ffb75e4e7aabdaa68e401b0a8c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fef88ebc9d96483d89abf198226b6b0f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"026ffc4ef9114a3fba1c63a8cbe887d7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9f259f93ab12482d828ba001c07d8328":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ba2d4d6d0844e76ba9287ecee700dc3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d7ad16911702422f830c4176734763c3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4981b58a651146c6a425493249e16af5","IPY_MODEL_1d60ebefa04542798877f18af81d7d79","IPY_MODEL_5c346304f098416b8fa6f5ec3353c69d"],"layout":"IPY_MODEL_e05f3695bd364c7d8a693bb8ea4c75cd"}},"4981b58a651146c6a425493249e16af5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e8c1308d3ff048f88ab649da09b66555","placeholder":"​","style":"IPY_MODEL_97c035bd666b44ce9d0149b2ae20b31b","value":"generation_config.json: 100%"}},"1d60ebefa04542798877f18af81d7d79":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5d7128e466224c0f8fa382c3a2e777dd","max":132,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cd5a3058ae694761abf6d196652ce977","value":132}},"5c346304f098416b8fa6f5ec3353c69d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cd6ae6a820f645fba18be8d77b397cb3","placeholder":"​","style":"IPY_MODEL_ce62e2b1e1d24221bdd22e68243b732d","value":" 132/132 [00:00&lt;00:00, 1.17kB/s]"}},"e05f3695bd364c7d8a693bb8ea4c75cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e8c1308d3ff048f88ab649da09b66555":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"97c035bd666b44ce9d0149b2ae20b31b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5d7128e466224c0f8fa382c3a2e777dd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd5a3058ae694761abf6d196652ce977":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cd6ae6a820f645fba18be8d77b397cb3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ce62e2b1e1d24221bdd22e68243b732d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9dd09658feab404295f4144442eadb4b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9929cf15a3de4f199f2d62db1ea03873","IPY_MODEL_7f79d6c1b15c49f2ab6954cd57f14178","IPY_MODEL_79299582d4234ac8896d3d22e1c41b4c"],"layout":"IPY_MODEL_14a2af406a6a45569a1d6f667b2242b3"}},"9929cf15a3de4f199f2d62db1ea03873":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e3ced63223f14b9d8890212f4a3a7bb3","placeholder":"​","style":"IPY_MODEL_9db512d7e2994531bdec22526a241e48","value":"config.json: 100%"}},"7f79d6c1b15c49f2ab6954cd57f14178":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0b76f3eceab641e49e286827747e6347","max":878,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4a4cb58437174cceb8e52782364fa3a0","value":878}},"79299582d4234ac8896d3d22e1c41b4c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9a8a0823d32444e6838abe2fe65515d9","placeholder":"​","style":"IPY_MODEL_3dc0d1015c25413c843581d8d47e7b4c","value":" 878/878 [00:00&lt;00:00, 6.25kB/s]"}},"14a2af406a6a45569a1d6f667b2242b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e3ced63223f14b9d8890212f4a3a7bb3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9db512d7e2994531bdec22526a241e48":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0b76f3eceab641e49e286827747e6347":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4a4cb58437174cceb8e52782364fa3a0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9a8a0823d32444e6838abe2fe65515d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3dc0d1015c25413c843581d8d47e7b4c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c0cf464f6a1d4ebb8b0c2db562a97426":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8af0245bc4474182850d7ddcb5898d45","IPY_MODEL_f0af35c3ee0644cc9c8991c50bd1f15b","IPY_MODEL_61c6a90f70994fad9d584e49de424235"],"layout":"IPY_MODEL_dfe834669ebe4500b2dc2a3027f24796"}},"8af0245bc4474182850d7ddcb5898d45":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7974448bc874460680956f19c217645e","placeholder":"​","style":"IPY_MODEL_50a8353c696b457daec07c7ccfd24ff4","value":"adapter_config.json: 100%"}},"f0af35c3ee0644cc9c8991c50bd1f15b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3f97f445644b4186a7fa21017f5d0da8","max":825,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6847ca87da2b4590967724d70cd0fe0e","value":825}},"61c6a90f70994fad9d584e49de424235":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d37a4af798f14586a8872a5a5f129e97","placeholder":"​","style":"IPY_MODEL_f15a4fa26d55475d9efac1102b466818","value":" 825/825 [00:00&lt;00:00, 5.11kB/s]"}},"dfe834669ebe4500b2dc2a3027f24796":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7974448bc874460680956f19c217645e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"50a8353c696b457daec07c7ccfd24ff4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3f97f445644b4186a7fa21017f5d0da8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6847ca87da2b4590967724d70cd0fe0e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d37a4af798f14586a8872a5a5f129e97":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f15a4fa26d55475d9efac1102b466818":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fc0a1e931f7745f1a9fc85eb5c1121b8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f5f4f0edbe6345f3b2a4145332e67751","IPY_MODEL_1e007a7354984c96a8a028a2ad09b917","IPY_MODEL_005caf6940294263ac19e6d4dec081b8"],"layout":"IPY_MODEL_487db5ac7fc847b194129fcdcef21c10"}},"f5f4f0edbe6345f3b2a4145332e67751":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_25cde76b693445a69f8d56b0a046ca1e","placeholder":"​","style":"IPY_MODEL_2dbb4be13bb5402ab65b1d1d063b2080","value":"chat_template.jinja: 100%"}},"1e007a7354984c96a8a028a2ad09b917":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2a07805375804dff9ee9b1e0364d6cc8","max":368,"min":0,"orientation":"horizontal","style":"IPY_MODEL_eea6e59f773e4890a93327ac89463e07","value":368}},"005caf6940294263ac19e6d4dec081b8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3ae61cecfd0548e19e0bfc5100185d1e","placeholder":"​","style":"IPY_MODEL_0106122a22484e02a779800d2f29c931","value":" 368/368 [00:00&lt;00:00, 1.82kB/s]"}},"487db5ac7fc847b194129fcdcef21c10":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"25cde76b693445a69f8d56b0a046ca1e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2dbb4be13bb5402ab65b1d1d063b2080":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2a07805375804dff9ee9b1e0364d6cc8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eea6e59f773e4890a93327ac89463e07":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3ae61cecfd0548e19e0bfc5100185d1e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0106122a22484e02a779800d2f29c931":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a686ec772c074753a639ef34e2d8ed89":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c51e7dfe72184aa2af3fa662968da02d","IPY_MODEL_ed9f979345ec4809bd0b70e40e503b7a","IPY_MODEL_c68c16f515cf4d1bb7646b80c702da0a"],"layout":"IPY_MODEL_5705219d707a40c383ac630f8c7aa48d"}},"c51e7dfe72184aa2af3fa662968da02d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_774021b7d4434634a46729405f709ae0","placeholder":"​","style":"IPY_MODEL_1cb0056e31eb41ef8a7e2fe040ac9768","value":"README.md: "}},"ed9f979345ec4809bd0b70e40e503b7a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dccfd783d6f44812b9c777b78791a70f","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c814050c705f4b708b0d19799c93602e","value":1}},"c68c16f515cf4d1bb7646b80c702da0a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1301c39087c6499b95b541bc5958a60b","placeholder":"​","style":"IPY_MODEL_5066b690d8174548b9bf792e49d48901","value":" 5.18k/? [00:00&lt;00:00, 44.4kB/s]"}},"5705219d707a40c383ac630f8c7aa48d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"774021b7d4434634a46729405f709ae0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1cb0056e31eb41ef8a7e2fe040ac9768":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dccfd783d6f44812b9c777b78791a70f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"c814050c705f4b708b0d19799c93602e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1301c39087c6499b95b541bc5958a60b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5066b690d8174548b9bf792e49d48901":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f243624ca1914af3a0f4dc519d4c95d3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_04b1e32a8fee4f79b2d66679faa69161","IPY_MODEL_4565026e0fc046418e28cb4beba4d567","IPY_MODEL_4bafdbbe845146dfa3073863df0f103a"],"layout":"IPY_MODEL_9c83ae11ed084f4f933ea2256e2d5f60"}},"04b1e32a8fee4f79b2d66679faa69161":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ff8819f990964d5fb3015b014b743057","placeholder":"​","style":"IPY_MODEL_72ea3ccd1fd040e1a1a8ad0d106aaffd","value":"merges.txt: "}},"4565026e0fc046418e28cb4beba4d567":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4af083c59c1e45cda35255f3a92e80c7","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bc1115aaa3ba45d780bba8cefd0a785c","value":1}},"4bafdbbe845146dfa3073863df0f103a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_be318294501747bf923faba63bbb59d9","placeholder":"​","style":"IPY_MODEL_f73f19a40051474d81c28badab60462d","value":" 466k/? [00:00&lt;00:00, 2.98MB/s]"}},"9c83ae11ed084f4f933ea2256e2d5f60":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff8819f990964d5fb3015b014b743057":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"72ea3ccd1fd040e1a1a8ad0d106aaffd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4af083c59c1e45cda35255f3a92e80c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"bc1115aaa3ba45d780bba8cefd0a785c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"be318294501747bf923faba63bbb59d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f73f19a40051474d81c28badab60462d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"31d038957c6d474399f78a668b208556":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fccc5b5410b84379af93728c1ae58149","IPY_MODEL_3ec2157448114749b1b2dd1cd2137b55","IPY_MODEL_8a28b66f179a4c23b62eb7c8d62f2956"],"layout":"IPY_MODEL_317f34dd5c644e6c8838c4e6ef329dbc"}},"fccc5b5410b84379af93728c1ae58149":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_562bf0e6509d4ae3bb166d9cf714d408","placeholder":"​","style":"IPY_MODEL_2e3f564d0cec41c98eed505592b6a342","value":"model.safetensors: 100%"}},"3ec2157448114749b1b2dd1cd2137b55":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f071356f34ef4f499bb1185d83c143bb","max":538090408,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1b95e355a2d64edebf361940ed45e671","value":538090408}},"8a28b66f179a4c23b62eb7c8d62f2956":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c5abe80fecc74f46ac37ace76e8e9283","placeholder":"​","style":"IPY_MODEL_13e1489bfb464bcf85d558a80f20c8ce","value":" 538M/538M [00:07&lt;00:00, 95.6MB/s]"}},"317f34dd5c644e6c8838c4e6ef329dbc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"562bf0e6509d4ae3bb166d9cf714d408":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e3f564d0cec41c98eed505592b6a342":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f071356f34ef4f499bb1185d83c143bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b95e355a2d64edebf361940ed45e671":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c5abe80fecc74f46ac37ace76e8e9283":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13e1489bfb464bcf85d558a80f20c8ce":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"05b6128436b242d8853875398a96121e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c7de1362d5ec419b868e26b7c1df01e4","IPY_MODEL_d496018663e748f38d4c2123fb13c581","IPY_MODEL_dbe8804d419e46ea96f41f0897bf5eae"],"layout":"IPY_MODEL_c57d8a3f1a77427797d888f5ea0bfdad"}},"c7de1362d5ec419b868e26b7c1df01e4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_228b7d2b4ec14e69be4e0ad8ba512fd1","placeholder":"​","style":"IPY_MODEL_efea2fdb2ff3495e95182d1041a04df2","value":"tokenizer.json: "}},"d496018663e748f38d4c2123fb13c581":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3b8954edc20f4e828c04770f912f2647","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_77e1f950d5ae463884589735551537c8","value":1}},"dbe8804d419e46ea96f41f0897bf5eae":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f6d04912976480291c330f8adc95be4","placeholder":"​","style":"IPY_MODEL_6f34f8a1158b4a5a832776e46334b83a","value":" 3.52M/? [00:00&lt;00:00, 1.91MB/s]"}},"c57d8a3f1a77427797d888f5ea0bfdad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"228b7d2b4ec14e69be4e0ad8ba512fd1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"efea2fdb2ff3495e95182d1041a04df2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3b8954edc20f4e828c04770f912f2647":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"77e1f950d5ae463884589735551537c8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7f6d04912976480291c330f8adc95be4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f34f8a1158b4a5a832776e46334b83a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fddb24fecfc24ab1af903c6e67eeb4c6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ea0f3fa2a8b44060a0f5621f445a3773","IPY_MODEL_d794d7910fcf4f2ca68be1e76c2c4693","IPY_MODEL_50b09eaf32f14ea6b53a23aa9d177cf9"],"layout":"IPY_MODEL_2b0b8f3e1b1347018f5d28ea5a33df6c"}},"ea0f3fa2a8b44060a0f5621f445a3773":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1345e3c8496f4cd9b6e57fcb8b6b91d1","placeholder":"​","style":"IPY_MODEL_e6d1c126d9464d06953f70e29799cd36","value":"tokenizer_config.json: "}},"d794d7910fcf4f2ca68be1e76c2c4693":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a85dadda1466433f88bf78671348e800","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_66b3b22ac2b34117be8e9d56a8ee1f0b","value":1}},"50b09eaf32f14ea6b53a23aa9d177cf9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e2dec1df2a8a4599a7995201639baddc","placeholder":"​","style":"IPY_MODEL_2c35438820f64b3abea54cfc339f30a7","value":" 3.40k/? [00:00&lt;00:00, 27.4kB/s]"}},"2b0b8f3e1b1347018f5d28ea5a33df6c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1345e3c8496f4cd9b6e57fcb8b6b91d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6d1c126d9464d06953f70e29799cd36":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a85dadda1466433f88bf78671348e800":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"66b3b22ac2b34117be8e9d56a8ee1f0b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e2dec1df2a8a4599a7995201639baddc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2c35438820f64b3abea54cfc339f30a7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eff4f98e8fb4451699ffa2de716ba7b6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4ae9f4afea214ea2abca6b3d8e7885bd","IPY_MODEL_9b02b31907bd4f7896444574bfc26656","IPY_MODEL_1d43a4bb49bd45cbae59af1fb043830d"],"layout":"IPY_MODEL_49427be632f74e249250bd72fd584c1b"}},"4ae9f4afea214ea2abca6b3d8e7885bd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9a09e461f1224b9f82063f959a46cffa","placeholder":"​","style":"IPY_MODEL_379faafb48a8443e8431a0319cb68b9f","value":"special_tokens_map.json: 100%"}},"9b02b31907bd4f7896444574bfc26656":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d2a31306eabc49b29b1a42a16964a47f","max":655,"min":0,"orientation":"horizontal","style":"IPY_MODEL_430f9874782045da8cf7b6abfea76b0a","value":655}},"1d43a4bb49bd45cbae59af1fb043830d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_917610aee7bd40ad8a7f43bb79c64abe","placeholder":"​","style":"IPY_MODEL_084847d645cc4f5090b60599a0da6eb8","value":" 655/655 [00:00&lt;00:00, 5.43kB/s]"}},"49427be632f74e249250bd72fd584c1b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a09e461f1224b9f82063f959a46cffa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"379faafb48a8443e8431a0319cb68b9f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d2a31306eabc49b29b1a42a16964a47f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"430f9874782045da8cf7b6abfea76b0a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"917610aee7bd40ad8a7f43bb79c64abe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"084847d645cc4f5090b60599a0da6eb8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"72b556c876714377ab68f4fa21891724":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2a7c5cc9f5e7474fb98d5c56bf3f4fe8","IPY_MODEL_7f7072ac112e49ae8073e338bb480818","IPY_MODEL_b0423195663c4946a97baf0c94d2cbd1"],"layout":"IPY_MODEL_82baa29cbcd349949c20b1ff710bda2c"}},"2a7c5cc9f5e7474fb98d5c56bf3f4fe8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_983afa65d9e048a78a126cf463b9db51","placeholder":"​","style":"IPY_MODEL_da2f096d785e47928d01a50fe83af5c3","value":"vocab.json: "}},"7f7072ac112e49ae8073e338bb480818":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e46ce72f287d415e998b182941ccdfcb","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1b81a29127674ed19907471c890e9d3a","value":1}},"b0423195663c4946a97baf0c94d2cbd1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_041aac97ba254f4ba5dd41bf544d2638","placeholder":"​","style":"IPY_MODEL_26fe7f58ce844018aa0493ea7a5035b2","value":" 801k/? [00:00&lt;00:00, 4.74MB/s]"}},"82baa29cbcd349949c20b1ff710bda2c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"983afa65d9e048a78a126cf463b9db51":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"da2f096d785e47928d01a50fe83af5c3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e46ce72f287d415e998b182941ccdfcb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"1b81a29127674ed19907471c890e9d3a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"041aac97ba254f4ba5dd41bf544d2638":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26fe7f58ce844018aa0493ea7a5035b2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2c003bc8ea2a4801aff62edc3523e31a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4a36081931ec4efe857dba76613087d9","IPY_MODEL_1742960f1c28414e820498402ec2ae3b","IPY_MODEL_a2a150d1544f4743923b9d033f9488a3"],"layout":"IPY_MODEL_d3335208d04c492ebe0b1821e2768187"}},"4a36081931ec4efe857dba76613087d9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2ce62dcc4d914d49968094a6998e8f25","placeholder":"​","style":"IPY_MODEL_ec3bc5be654342b19f0c99c9022138ba","value":"config.json: 100%"}},"1742960f1c28414e820498402ec2ae3b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_20fa020dcf0b4f1b98b88d7f0cdf1591","max":861,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3523090983844758820167b8e3c16c41","value":861}},"a2a150d1544f4743923b9d033f9488a3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_02decf6fa1a34407b9559ceb84d5a72f","placeholder":"​","style":"IPY_MODEL_7a0894060a6848c2bda23c48d1c0c828","value":" 861/861 [00:00&lt;00:00, 39.9kB/s]"}},"d3335208d04c492ebe0b1821e2768187":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ce62dcc4d914d49968094a6998e8f25":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec3bc5be654342b19f0c99c9022138ba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"20fa020dcf0b4f1b98b88d7f0cdf1591":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3523090983844758820167b8e3c16c41":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"02decf6fa1a34407b9559ceb84d5a72f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7a0894060a6848c2bda23c48d1c0c828":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"113a82423f9a4e949b326d32b4250702":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_207f54543fdd4b55ad4cdd2008f5e674","IPY_MODEL_cfcf2943646f496a8b6893252a0a9bbe","IPY_MODEL_c58222ceff724ca28ccf7e8cf4878912"],"layout":"IPY_MODEL_4aef5da9239545aca1a3ca8398e26fdb"}},"207f54543fdd4b55ad4cdd2008f5e674":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6b089716a7d04a7bab739d429d9f0d67","placeholder":"​","style":"IPY_MODEL_709be19a5d1d4921b9049ba9c83e878f","value":"model.safetensors: 100%"}},"cfcf2943646f496a8b6893252a0a9bbe":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_04beff22171a40999b891b8a995281dd","max":269060552,"min":0,"orientation":"horizontal","style":"IPY_MODEL_828475fcc3f7444bbe45019c878e726e","value":269060552}},"c58222ceff724ca28ccf7e8cf4878912":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3086b494a39a43939d9d0231ed319f75","placeholder":"​","style":"IPY_MODEL_0fdcebbb74c6453a90c10a315869f5f6","value":" 269M/269M [00:03&lt;00:00, 87.1MB/s]"}},"4aef5da9239545aca1a3ca8398e26fdb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b089716a7d04a7bab739d429d9f0d67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"709be19a5d1d4921b9049ba9c83e878f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"04beff22171a40999b891b8a995281dd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"828475fcc3f7444bbe45019c878e726e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3086b494a39a43939d9d0231ed319f75":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0fdcebbb74c6453a90c10a315869f5f6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"51edee1e43e3446a950e0023cac18bfa":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_681bc5aa718545b3bb8fadb3123e145a","IPY_MODEL_ada1097bc7cb4ffc8b832fed275c8d7a","IPY_MODEL_02d474f019ea43db895c3136e3c6612c"],"layout":"IPY_MODEL_253f13f1f97d433c84ac91f89ccc20ea"}},"681bc5aa718545b3bb8fadb3123e145a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7c2986e2e4bc442b93aece614591af4e","placeholder":"​","style":"IPY_MODEL_0f60c64cc4234c07b4e286fe536f88c7","value":"generation_config.json: 100%"}},"ada1097bc7cb4ffc8b832fed275c8d7a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a441a521e4004d35997463a2c7f6100e","max":132,"min":0,"orientation":"horizontal","style":"IPY_MODEL_eb02aa7da3a047fe89cb613c8589e14b","value":132}},"02d474f019ea43db895c3136e3c6612c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_94aaf1198c134c139b2e5a208aee9f4c","placeholder":"​","style":"IPY_MODEL_2884221e80ab4288a99f152eac593098","value":" 132/132 [00:00&lt;00:00, 10.6kB/s]"}},"253f13f1f97d433c84ac91f89ccc20ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c2986e2e4bc442b93aece614591af4e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f60c64cc4234c07b4e286fe536f88c7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a441a521e4004d35997463a2c7f6100e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb02aa7da3a047fe89cb613c8589e14b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"94aaf1198c134c139b2e5a208aee9f4c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2884221e80ab4288a99f152eac593098":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# Conversione a GGUF\n","GGUF (GPT-Generated Unified Format) è un formato di file progettato per archiviare modelli di grandi dimensioni, ottimizzato per l'efficienza e la compatibilità con l'hardware consumer.\n","\n","È di fatto un formato molto comodo se vogliamo eseguire il nostro modello su hardware consumer, su server CPU only o hardware a basso costo (es: raspberry, mini pc, ecc...).\n","\n","Il modo più semplice per leggere/scrivere questo formato è utilizzare la famosa libreria llama_cpp.\n","\n","Questa libreria, oltre alla semplice conversione, permette anche di effettuare la quantizzazione, tecnica che permette di aumentare le performance del modello con un trade-off sulla sua precisione.\n","\n","In questo esercizio proviamo a convertire il modello addestrato nell'esercizio di function calling in formato gguf.\n","\n","Prima di tutto installiamo le dipendenze."],"metadata":{"id":"mhpG-Fj5J05R"}},{"cell_type":"code","source":["!pip install huggingface-hub numpy torch transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nHYV1aqj6ekR","executionInfo":{"status":"ok","timestamp":1751640904233,"user_tz":-120,"elapsed":124371,"user":{"displayName":"Nicolò Festa","userId":"09455685607482603239"}},"outputId":"a7d228b3-7d10-4c5b-cada-c15207a90290"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (0.33.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (3.18.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (2025.3.2)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (4.67.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (4.14.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (1.1.5)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (2025.6.15)\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"]}]},{"cell_type":"markdown","source":["Importiamo le librerie e accediamo a hugging face con la nostra API Key."],"metadata":{"id":"qLSyEerRorym"}},{"cell_type":"code","source":["import os\n","import subprocess\n","from huggingface_hub import snapshot_download, login\n","\n","login()"],"metadata":{"id":"J_zxPZXaqhSy","colab":{"base_uri":"https://localhost:8080/","height":17,"referenced_widgets":["589bbddf090e44a68d1489d87d566dfe","ab82c17455884725a9cdab8db298ef96","a24ee43e31e747efa49225db96764c6f","6181905329ab44c3b19e1d8390f364af","dc551824977248258e8e60f6abccbef3","a0ac5feecf9247f388abcad004231494","4a714d3a2d5342b58181bde2b59cadb8","fc68682e506c497faf26a4f1bf08a295","162977cb0e86462dab260f4a4aa8f25e","0e509d085e694952a6c891213faeb631","075612931faa47b1ab3a8a5a6c679b6e","67de91bc52514fec9c31d7e8b151cbfe","78142030b734446cb488096e4055d048","b9a4d1d3a05a42d4924efc785247bd9a","8db16a9d276d456ebcd626480fb8b134","52fb75ef1e0443be875f83a4e2f3206c","ded023b4b22e4fcdbf50b987b602931b","6882db340a794962aae9a141ad8c177d","d01f8b682d2e419580fd186fb2e8ee46","8b77ab6cc3d14070a6ea0b599923d828"]},"executionInfo":{"status":"ok","timestamp":1751640909669,"user_tz":-120,"elapsed":2668,"user":{"displayName":"Nicolò Festa","userId":"09455685607482603239"}},"outputId":"a5b0befe-c7cd-40bb-89bb-ecda39eda417"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"589bbddf090e44a68d1489d87d566dfe"}},"metadata":{}}]},{"cell_type":"markdown","source":["Definiamo una funzione che si scarica la repository di llama.cpp. Questo perché nella repository è presente uno script che permette di convertire dal formato di hugging face a gguf."],"metadata":{"id":"woZm_3pRoPUJ"}},{"cell_type":"code","source":["def clone_llama_cpp():\n","    \"\"\"Clone llama.cpp repository if not already present.\"\"\"\n","    llama_cpp_dir = os.path.abspath(os.path.join('.', \"llama.cpp\"))\n","    if not os.path.exists(llama_cpp_dir):\n","        print(\"Cloning llama.cpp repository...\")\n","        subprocess.run(\n","            [\"git\", \"clone\", \"https://github.com/ggerganov/llama.cpp.git\", llama_cpp_dir],\n","            check=True\n","        )\n","    return llama_cpp_dir"],"metadata":{"id":"BALvlzdxqrdW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ora definiamo una funzione che esegua quello script invocando un processo fork di python."],"metadata":{"id":"Ayu2A0UuoeJ4"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"u1y8R5Ne_0DD"},"outputs":[],"source":["def convert_to_gguf(model_path: str, output_path: str, quantization: str = \"q4_k_m\"):\n","    \"\"\"\n","    Convert a PyTorch model to GGUF format using llama.cpp's conversion tools.\n","\n","    Args:\n","        model_path: Path to the merged LoRA model\n","        output_path: Path where the GGUF model will be saved\n","        quantization: Quantization method to use (e.g., \"q4_k_m\", \"q5_k_m\", \"q8_0\")\n","    \"\"\"\n","    # Convert paths to absolute\n","    model_path = os.path.abspath(model_path)\n","    output_path = os.path.abspath(output_path)\n","    os.makedirs(output_path, exist_ok=True)\n","\n","    print(f\"Converting model from {model_path} to GGUF format...\")\n","    print(f\"Model directory contents:\")\n","    for item in os.listdir(model_path):\n","        print(f\"- {item}\")\n","\n","    # Get llama.cpp repository\n","    llama_cpp_dir = clone_llama_cpp()\n","\n","    # Convert to GGUF using llama.cpp's convert_hf_to_gguf.py\n","    print(\"Converting to GGUF format...\")\n","    gguf_path = os.path.join(output_path, f\"model-{quantization}.gguf\")\n","\n","    # Run the conversion script\n","    convert_cmd = [\n","        \"python3\",\n","        os.path.join(llama_cpp_dir, \"convert_hf_to_gguf.py\"),\n","        \"--outfile\", gguf_path,\n","        \"--outtype\", quantization,\n","        \"--verbose\",  # Add verbose flag to see more details\n","        model_path  # Use absolute path to model directory\n","    ]\n","\n","    try:\n","        # Change to llama.cpp directory for conversion\n","        original_dir = os.getcwd()\n","        os.chdir(llama_cpp_dir)\n","\n","        # Install required dependencies\n","        print(\"Installing llama.cpp dependencies...\")\n","        subprocess.run([\"pip\", \"install\", \"-r\", \"requirements.txt\"], check=True)\n","\n","        # Run conversion with captured output\n","        print(\"Running conversion script...\")\n","        print(f\"Command: {' '.join(convert_cmd)}\")\n","        result = subprocess.run(\n","            convert_cmd,\n","            capture_output=True,\n","            text=True,\n","            check=False  # Don't raise exception immediately\n","        )\n","\n","        # Print the output regardless of success/failure\n","        if result.stdout:\n","            print(\"Conversion script output:\")\n","            print(result.stdout)\n","        if result.stderr:\n","            print(\"Conversion script errors:\")\n","            print(result.stderr)\n","\n","        # Now check if the command was successful\n","        if result.returncode != 0:\n","            raise subprocess.CalledProcessError(\n","                result.returncode,\n","                convert_cmd,\n","                output=result.stdout,\n","                stderr=result.stderr\n","            )\n","\n","        # Change back to original directory\n","        os.chdir(original_dir)\n","\n","        print(f\"Successfully converted model to GGUF format: {gguf_path}\")\n","        return gguf_path\n","    except subprocess.CalledProcessError as e:\n","        print(f\"Error during conversion: {e}\")\n","        raise\n","    except Exception as e:\n","        print(f\"Unexpected error: {e}\")\n","        raise\n","    finally:\n","        # Ensure we return to the original directory\n","        os.chdir(original_dir)\n"]},{"cell_type":"markdown","source":["Scarichiamo il modello che abbiamo addestrato prima dalla repository di hugging face.\n","\n","Con questo comando otteniamo il percorso della cartella temporanea in cui viene salvato il modello."],"metadata":{"id":"bJiBBGs4o1_1"}},{"cell_type":"code","source":["model_path = snapshot_download(repo_id=\"monadestudio/smol-function-calling\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":606,"referenced_widgets":["01640ce31be34053a08820ed24dd3c3b","78f06655bc4c425e8ff3555af7bdfe80","9724992f57904465b001e2b20874eafb","3f9c4198d05545d2860058d6358b2c8e","355dafac28db419fb33467a7bfd119b1","0967dad5e68748e1b3d1cf5ff0e09f39","194ab9ed3612455baaa97bbc4b4a7fe8","d20b3d425e89418b8ba4173da9e6b351","35acae95a6584f44b7b8362d7a345a82","a600618f6c7048b0b28e2d890e9ebbed","c4e616b1d17b407986809eae73e4b0f2","b632f12aa4f143d48a5a429c90ea88a9","6f6dc6da3e2c4fc5bfa6a6e2d071b4eb","efeffde1e060494794a8f930c47e520f","36a0b3f0e7344c76b1e26ecef9cb7a7b","e96581c1d8304bc2a61637b3740b6f10","685e9159a49d4bebaf4bbcd220839623","86df176c06d54127b13e6f23d49f0d31","cf158d9cc40c4393b58c011b4fd24e21","2eb97bbd40514969a98c64c961f45433","a9f91d7695f04e3b84e5a80cbd9b966f","361a49287fa34c648931f75147173249","1519d7252c3243499aa65de7db9682db","cf7e87353fcd488e8b26c28c27b3fcdf","2431a1f76ba149aba4c8fd3ab8cb1fa4","0fc6a0c7cafa426382affb03897e8d8e","ee36d175ca9847e9b70f310edcf393b2","0cdbc154674542bc9744fd8d664717d7","0cf3e4ffb75e4e7aabdaa68e401b0a8c","fef88ebc9d96483d89abf198226b6b0f","026ffc4ef9114a3fba1c63a8cbe887d7","9f259f93ab12482d828ba001c07d8328","1ba2d4d6d0844e76ba9287ecee700dc3","d7ad16911702422f830c4176734763c3","4981b58a651146c6a425493249e16af5","1d60ebefa04542798877f18af81d7d79","5c346304f098416b8fa6f5ec3353c69d","e05f3695bd364c7d8a693bb8ea4c75cd","e8c1308d3ff048f88ab649da09b66555","97c035bd666b44ce9d0149b2ae20b31b","5d7128e466224c0f8fa382c3a2e777dd","cd5a3058ae694761abf6d196652ce977","cd6ae6a820f645fba18be8d77b397cb3","ce62e2b1e1d24221bdd22e68243b732d","9dd09658feab404295f4144442eadb4b","9929cf15a3de4f199f2d62db1ea03873","7f79d6c1b15c49f2ab6954cd57f14178","79299582d4234ac8896d3d22e1c41b4c","14a2af406a6a45569a1d6f667b2242b3","e3ced63223f14b9d8890212f4a3a7bb3","9db512d7e2994531bdec22526a241e48","0b76f3eceab641e49e286827747e6347","4a4cb58437174cceb8e52782364fa3a0","9a8a0823d32444e6838abe2fe65515d9","3dc0d1015c25413c843581d8d47e7b4c","c0cf464f6a1d4ebb8b0c2db562a97426","8af0245bc4474182850d7ddcb5898d45","f0af35c3ee0644cc9c8991c50bd1f15b","61c6a90f70994fad9d584e49de424235","dfe834669ebe4500b2dc2a3027f24796","7974448bc874460680956f19c217645e","50a8353c696b457daec07c7ccfd24ff4","3f97f445644b4186a7fa21017f5d0da8","6847ca87da2b4590967724d70cd0fe0e","d37a4af798f14586a8872a5a5f129e97","f15a4fa26d55475d9efac1102b466818","fc0a1e931f7745f1a9fc85eb5c1121b8","f5f4f0edbe6345f3b2a4145332e67751","1e007a7354984c96a8a028a2ad09b917","005caf6940294263ac19e6d4dec081b8","487db5ac7fc847b194129fcdcef21c10","25cde76b693445a69f8d56b0a046ca1e","2dbb4be13bb5402ab65b1d1d063b2080","2a07805375804dff9ee9b1e0364d6cc8","eea6e59f773e4890a93327ac89463e07","3ae61cecfd0548e19e0bfc5100185d1e","0106122a22484e02a779800d2f29c931","a686ec772c074753a639ef34e2d8ed89","c51e7dfe72184aa2af3fa662968da02d","ed9f979345ec4809bd0b70e40e503b7a","c68c16f515cf4d1bb7646b80c702da0a","5705219d707a40c383ac630f8c7aa48d","774021b7d4434634a46729405f709ae0","1cb0056e31eb41ef8a7e2fe040ac9768","dccfd783d6f44812b9c777b78791a70f","c814050c705f4b708b0d19799c93602e","1301c39087c6499b95b541bc5958a60b","5066b690d8174548b9bf792e49d48901","f243624ca1914af3a0f4dc519d4c95d3","04b1e32a8fee4f79b2d66679faa69161","4565026e0fc046418e28cb4beba4d567","4bafdbbe845146dfa3073863df0f103a","9c83ae11ed084f4f933ea2256e2d5f60","ff8819f990964d5fb3015b014b743057","72ea3ccd1fd040e1a1a8ad0d106aaffd","4af083c59c1e45cda35255f3a92e80c7","bc1115aaa3ba45d780bba8cefd0a785c","be318294501747bf923faba63bbb59d9","f73f19a40051474d81c28badab60462d","31d038957c6d474399f78a668b208556","fccc5b5410b84379af93728c1ae58149","3ec2157448114749b1b2dd1cd2137b55","8a28b66f179a4c23b62eb7c8d62f2956","317f34dd5c644e6c8838c4e6ef329dbc","562bf0e6509d4ae3bb166d9cf714d408","2e3f564d0cec41c98eed505592b6a342","f071356f34ef4f499bb1185d83c143bb","1b95e355a2d64edebf361940ed45e671","c5abe80fecc74f46ac37ace76e8e9283","13e1489bfb464bcf85d558a80f20c8ce","05b6128436b242d8853875398a96121e","c7de1362d5ec419b868e26b7c1df01e4","d496018663e748f38d4c2123fb13c581","dbe8804d419e46ea96f41f0897bf5eae","c57d8a3f1a77427797d888f5ea0bfdad","228b7d2b4ec14e69be4e0ad8ba512fd1","efea2fdb2ff3495e95182d1041a04df2","3b8954edc20f4e828c04770f912f2647","77e1f950d5ae463884589735551537c8","7f6d04912976480291c330f8adc95be4","6f34f8a1158b4a5a832776e46334b83a","fddb24fecfc24ab1af903c6e67eeb4c6","ea0f3fa2a8b44060a0f5621f445a3773","d794d7910fcf4f2ca68be1e76c2c4693","50b09eaf32f14ea6b53a23aa9d177cf9","2b0b8f3e1b1347018f5d28ea5a33df6c","1345e3c8496f4cd9b6e57fcb8b6b91d1","e6d1c126d9464d06953f70e29799cd36","a85dadda1466433f88bf78671348e800","66b3b22ac2b34117be8e9d56a8ee1f0b","e2dec1df2a8a4599a7995201639baddc","2c35438820f64b3abea54cfc339f30a7","eff4f98e8fb4451699ffa2de716ba7b6","4ae9f4afea214ea2abca6b3d8e7885bd","9b02b31907bd4f7896444574bfc26656","1d43a4bb49bd45cbae59af1fb043830d","49427be632f74e249250bd72fd584c1b","9a09e461f1224b9f82063f959a46cffa","379faafb48a8443e8431a0319cb68b9f","d2a31306eabc49b29b1a42a16964a47f","430f9874782045da8cf7b6abfea76b0a","917610aee7bd40ad8a7f43bb79c64abe","084847d645cc4f5090b60599a0da6eb8","72b556c876714377ab68f4fa21891724","2a7c5cc9f5e7474fb98d5c56bf3f4fe8","7f7072ac112e49ae8073e338bb480818","b0423195663c4946a97baf0c94d2cbd1","82baa29cbcd349949c20b1ff710bda2c","983afa65d9e048a78a126cf463b9db51","da2f096d785e47928d01a50fe83af5c3","e46ce72f287d415e998b182941ccdfcb","1b81a29127674ed19907471c890e9d3a","041aac97ba254f4ba5dd41bf544d2638","26fe7f58ce844018aa0493ea7a5035b2"]},"id":"md2TeGT9xdpo","executionInfo":{"status":"ok","timestamp":1751640938191,"user_tz":-120,"elapsed":8862,"user":{"displayName":"Nicolò Festa","userId":"09455685607482603239"}},"outputId":"f78ee78f-d1cd-433b-e6d2-f44762ca4a6d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01640ce31be34053a08820ed24dd3c3b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["adapter_model.safetensors:   0%|          | 0.00/40.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b632f12aa4f143d48a5a429c90ea88a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":[".gitattributes: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1519d7252c3243499aa65de7db9682db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7ad16911702422f830c4176734763c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9dd09658feab404295f4144442eadb4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["adapter_config.json:   0%|          | 0.00/825 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0cf464f6a1d4ebb8b0c2db562a97426"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["chat_template.jinja:   0%|          | 0.00/368 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc0a1e931f7745f1a9fc85eb5c1121b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["README.md: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a686ec772c074753a639ef34e2d8ed89"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f243624ca1914af3a0f4dc519d4c95d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/538M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31d038957c6d474399f78a668b208556"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05b6128436b242d8853875398a96121e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fddb24fecfc24ab1af903c6e67eeb4c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/655 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eff4f98e8fb4451699ffa2de716ba7b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72b556c876714377ab68f4fa21891724"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["/root/.cache/huggingface/hub/models--monadestudio--smol-function-calling/snapshots/76ae67811058acd154412ff756dcaa3d21e398b9\n"]}]},{"cell_type":"markdown","source":["Proviamo se funziona correttamente prima della conversione:"],"metadata":{"id":"HWMM6Qzv6v1I"}},{"cell_type":"code","source":["# Importiamo le dipendenze\n","from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n","import torch\n","\n","# Definiamo il dispositivo su cui eseguire il modello in base alla disponibilità.\n","# CUDA (GPU), MPS (Apple Silicon) o CPU.\n","device = (\n","    \"cuda\"\n","    if torch.cuda.is_available()\n","    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",")\n","\n","# Scarichiamo il modello\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_path,\n","    torch_dtype=torch.float32,\n","    device_map=device,\n",")\n","\n","# Scarichiamo il tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_path)\n","\n","def generate_response(model, tokenizer, prompt, system_prompt=None):\n","    # Preparo il prompt in modalità chat e lo formatto in modo che il modello possa leggerlo.\n","    messages = [{\"role\": \"user\", \"content\": prompt}]\n","    # Se è presente un system prompt, lo aggiungo alla lista dei messaggi.\n","    if system_prompt:\n","        messages.insert(0, {\"role\": \"system\", \"content\": system_prompt})\n","    formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n","    # Tokenizzo il prompt\n","    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n","    # Lo passo al modello e gli chiedo di generare una risposta.\n","    outputs = model.generate(**inputs, max_new_tokens=512)\n","    # Decodifico la risposta del modello.\n","    output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    # Stampo la risposta, prendendo l'ultima parte della risposta del modello.\n","    return output.split(\"assistant\")[-1].strip()\n","\n","print(generate_response(model, tokenizer, \"What's the capital of France?\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":202,"referenced_widgets":["2c003bc8ea2a4801aff62edc3523e31a","4a36081931ec4efe857dba76613087d9","1742960f1c28414e820498402ec2ae3b","a2a150d1544f4743923b9d033f9488a3","d3335208d04c492ebe0b1821e2768187","2ce62dcc4d914d49968094a6998e8f25","ec3bc5be654342b19f0c99c9022138ba","20fa020dcf0b4f1b98b88d7f0cdf1591","3523090983844758820167b8e3c16c41","02decf6fa1a34407b9559ceb84d5a72f","7a0894060a6848c2bda23c48d1c0c828","113a82423f9a4e949b326d32b4250702","207f54543fdd4b55ad4cdd2008f5e674","cfcf2943646f496a8b6893252a0a9bbe","c58222ceff724ca28ccf7e8cf4878912","4aef5da9239545aca1a3ca8398e26fdb","6b089716a7d04a7bab739d429d9f0d67","709be19a5d1d4921b9049ba9c83e878f","04beff22171a40999b891b8a995281dd","828475fcc3f7444bbe45019c878e726e","3086b494a39a43939d9d0231ed319f75","0fdcebbb74c6453a90c10a315869f5f6","51edee1e43e3446a950e0023cac18bfa","681bc5aa718545b3bb8fadb3123e145a","ada1097bc7cb4ffc8b832fed275c8d7a","02d474f019ea43db895c3136e3c6612c","253f13f1f97d433c84ac91f89ccc20ea","7c2986e2e4bc442b93aece614591af4e","0f60c64cc4234c07b4e286fe536f88c7","a441a521e4004d35997463a2c7f6100e","eb02aa7da3a047fe89cb613c8589e14b","94aaf1198c134c139b2e5a208aee9f4c","2884221e80ab4288a99f152eac593098"]},"id":"0486oGqz6vMI","executionInfo":{"status":"ok","timestamp":1751641006688,"user_tz":-120,"elapsed":61325,"user":{"displayName":"Nicolò Festa","userId":"09455685607482603239"}},"outputId":"d68bfbbf-b872-41af-e3dc-8dc4110ad4d1"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/861 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c003bc8ea2a4801aff62edc3523e31a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/269M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"113a82423f9a4e949b326d32b4250702"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51edee1e43e3446a950e0023cac18bfa"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Loading adapter weights from /root/.cache/huggingface/hub/models--monadestudio--smol-function-calling/snapshots/76ae67811058acd154412ff756dcaa3d21e398b9 led to missing keys in the model: model.layers.0.self_attn.q_proj.lora_A.default.weight, model.layers.0.self_attn.q_proj.lora_B.default.weight, model.layers.0.self_attn.k_proj.lora_A.default.weight, model.layers.0.self_attn.k_proj.lora_B.default.weight, model.layers.0.self_attn.v_proj.lora_A.default.weight, model.layers.0.self_attn.v_proj.lora_B.default.weight, model.layers.0.self_attn.o_proj.lora_A.default.weight, model.layers.0.self_attn.o_proj.lora_B.default.weight, model.layers.1.self_attn.q_proj.lora_A.default.weight, model.layers.1.self_attn.q_proj.lora_B.default.weight, model.layers.1.self_attn.k_proj.lora_A.default.weight, model.layers.1.self_attn.k_proj.lora_B.default.weight, model.layers.1.self_attn.v_proj.lora_A.default.weight, model.layers.1.self_attn.v_proj.lora_B.default.weight, model.layers.1.self_attn.o_proj.lora_A.default.weight, model.layers.1.self_attn.o_proj.lora_B.default.weight, model.layers.2.self_attn.q_proj.lora_A.default.weight, model.layers.2.self_attn.q_proj.lora_B.default.weight, model.layers.2.self_attn.k_proj.lora_A.default.weight, model.layers.2.self_attn.k_proj.lora_B.default.weight, model.layers.2.self_attn.v_proj.lora_A.default.weight, model.layers.2.self_attn.v_proj.lora_B.default.weight, model.layers.2.self_attn.o_proj.lora_A.default.weight, model.layers.2.self_attn.o_proj.lora_B.default.weight, model.layers.3.self_attn.q_proj.lora_A.default.weight, model.layers.3.self_attn.q_proj.lora_B.default.weight, model.layers.3.self_attn.k_proj.lora_A.default.weight, model.layers.3.self_attn.k_proj.lora_B.default.weight, model.layers.3.self_attn.v_proj.lora_A.default.weight, model.layers.3.self_attn.v_proj.lora_B.default.weight, model.layers.3.self_attn.o_proj.lora_A.default.weight, model.layers.3.self_attn.o_proj.lora_B.default.weight, model.layers.4.self_attn.q_proj.lora_A.default.weight, model.layers.4.self_attn.q_proj.lora_B.default.weight, model.layers.4.self_attn.k_proj.lora_A.default.weight, model.layers.4.self_attn.k_proj.lora_B.default.weight, model.layers.4.self_attn.v_proj.lora_A.default.weight, model.layers.4.self_attn.v_proj.lora_B.default.weight, model.layers.4.self_attn.o_proj.lora_A.default.weight, model.layers.4.self_attn.o_proj.lora_B.default.weight, model.layers.5.self_attn.q_proj.lora_A.default.weight, model.layers.5.self_attn.q_proj.lora_B.default.weight, model.layers.5.self_attn.k_proj.lora_A.default.weight, model.layers.5.self_attn.k_proj.lora_B.default.weight, model.layers.5.self_attn.v_proj.lora_A.default.weight, model.layers.5.self_attn.v_proj.lora_B.default.weight, model.layers.5.self_attn.o_proj.lora_A.default.weight, model.layers.5.self_attn.o_proj.lora_B.default.weight, model.layers.6.self_attn.q_proj.lora_A.default.weight, model.layers.6.self_attn.q_proj.lora_B.default.weight, model.layers.6.self_attn.k_proj.lora_A.default.weight, model.layers.6.self_attn.k_proj.lora_B.default.weight, model.layers.6.self_attn.v_proj.lora_A.default.weight, model.layers.6.self_attn.v_proj.lora_B.default.weight, model.layers.6.self_attn.o_proj.lora_A.default.weight, model.layers.6.self_attn.o_proj.lora_B.default.weight, model.layers.7.self_attn.q_proj.lora_A.default.weight, model.layers.7.self_attn.q_proj.lora_B.default.weight, model.layers.7.self_attn.k_proj.lora_A.default.weight, model.layers.7.self_attn.k_proj.lora_B.default.weight, model.layers.7.self_attn.v_proj.lora_A.default.weight, model.layers.7.self_attn.v_proj.lora_B.default.weight, model.layers.7.self_attn.o_proj.lora_A.default.weight, model.layers.7.self_attn.o_proj.lora_B.default.weight, model.layers.8.self_attn.q_proj.lora_A.default.weight, model.layers.8.self_attn.q_proj.lora_B.default.weight, model.layers.8.self_attn.k_proj.lora_A.default.weight, model.layers.8.self_attn.k_proj.lora_B.default.weight, model.layers.8.self_attn.v_proj.lora_A.default.weight, model.layers.8.self_attn.v_proj.lora_B.default.weight, model.layers.8.self_attn.o_proj.lora_A.default.weight, model.layers.8.self_attn.o_proj.lora_B.default.weight, model.layers.9.self_attn.q_proj.lora_A.default.weight, model.layers.9.self_attn.q_proj.lora_B.default.weight, model.layers.9.self_attn.k_proj.lora_A.default.weight, model.layers.9.self_attn.k_proj.lora_B.default.weight, model.layers.9.self_attn.v_proj.lora_A.default.weight, model.layers.9.self_attn.v_proj.lora_B.default.weight, model.layers.9.self_attn.o_proj.lora_A.default.weight, model.layers.9.self_attn.o_proj.lora_B.default.weight, model.layers.10.self_attn.q_proj.lora_A.default.weight, model.layers.10.self_attn.q_proj.lora_B.default.weight, model.layers.10.self_attn.k_proj.lora_A.default.weight, model.layers.10.self_attn.k_proj.lora_B.default.weight, model.layers.10.self_attn.v_proj.lora_A.default.weight, model.layers.10.self_attn.v_proj.lora_B.default.weight, model.layers.10.self_attn.o_proj.lora_A.default.weight, model.layers.10.self_attn.o_proj.lora_B.default.weight, model.layers.11.self_attn.q_proj.lora_A.default.weight, model.layers.11.self_attn.q_proj.lora_B.default.weight, model.layers.11.self_attn.k_proj.lora_A.default.weight, model.layers.11.self_attn.k_proj.lora_B.default.weight, model.layers.11.self_attn.v_proj.lora_A.default.weight, model.layers.11.self_attn.v_proj.lora_B.default.weight, model.layers.11.self_attn.o_proj.lora_A.default.weight, model.layers.11.self_attn.o_proj.lora_B.default.weight, model.layers.12.self_attn.q_proj.lora_A.default.weight, model.layers.12.self_attn.q_proj.lora_B.default.weight, model.layers.12.self_attn.k_proj.lora_A.default.weight, model.layers.12.self_attn.k_proj.lora_B.default.weight, model.layers.12.self_attn.v_proj.lora_A.default.weight, model.layers.12.self_attn.v_proj.lora_B.default.weight, model.layers.12.self_attn.o_proj.lora_A.default.weight, model.layers.12.self_attn.o_proj.lora_B.default.weight, model.layers.13.self_attn.q_proj.lora_A.default.weight, model.layers.13.self_attn.q_proj.lora_B.default.weight, model.layers.13.self_attn.k_proj.lora_A.default.weight, model.layers.13.self_attn.k_proj.lora_B.default.weight, model.layers.13.self_attn.v_proj.lora_A.default.weight, model.layers.13.self_attn.v_proj.lora_B.default.weight, model.layers.13.self_attn.o_proj.lora_A.default.weight, model.layers.13.self_attn.o_proj.lora_B.default.weight, model.layers.14.self_attn.q_proj.lora_A.default.weight, model.layers.14.self_attn.q_proj.lora_B.default.weight, model.layers.14.self_attn.k_proj.lora_A.default.weight, model.layers.14.self_attn.k_proj.lora_B.default.weight, model.layers.14.self_attn.v_proj.lora_A.default.weight, model.layers.14.self_attn.v_proj.lora_B.default.weight, model.layers.14.self_attn.o_proj.lora_A.default.weight, model.layers.14.self_attn.o_proj.lora_B.default.weight, model.layers.15.self_attn.q_proj.lora_A.default.weight, model.layers.15.self_attn.q_proj.lora_B.default.weight, model.layers.15.self_attn.k_proj.lora_A.default.weight, model.layers.15.self_attn.k_proj.lora_B.default.weight, model.layers.15.self_attn.v_proj.lora_A.default.weight, model.layers.15.self_attn.v_proj.lora_B.default.weight, model.layers.15.self_attn.o_proj.lora_A.default.weight, model.layers.15.self_attn.o_proj.lora_B.default.weight, model.layers.16.self_attn.q_proj.lora_A.default.weight, model.layers.16.self_attn.q_proj.lora_B.default.weight, model.layers.16.self_attn.k_proj.lora_A.default.weight, model.layers.16.self_attn.k_proj.lora_B.default.weight, model.layers.16.self_attn.v_proj.lora_A.default.weight, model.layers.16.self_attn.v_proj.lora_B.default.weight, model.layers.16.self_attn.o_proj.lora_A.default.weight, model.layers.16.self_attn.o_proj.lora_B.default.weight, model.layers.17.self_attn.q_proj.lora_A.default.weight, model.layers.17.self_attn.q_proj.lora_B.default.weight, model.layers.17.self_attn.k_proj.lora_A.default.weight, model.layers.17.self_attn.k_proj.lora_B.default.weight, model.layers.17.self_attn.v_proj.lora_A.default.weight, model.layers.17.self_attn.v_proj.lora_B.default.weight, model.layers.17.self_attn.o_proj.lora_A.default.weight, model.layers.17.self_attn.o_proj.lora_B.default.weight, model.layers.18.self_attn.q_proj.lora_A.default.weight, model.layers.18.self_attn.q_proj.lora_B.default.weight, model.layers.18.self_attn.k_proj.lora_A.default.weight, model.layers.18.self_attn.k_proj.lora_B.default.weight, model.layers.18.self_attn.v_proj.lora_A.default.weight, model.layers.18.self_attn.v_proj.lora_B.default.weight, model.layers.18.self_attn.o_proj.lora_A.default.weight, model.layers.18.self_attn.o_proj.lora_B.default.weight, model.layers.19.self_attn.q_proj.lora_A.default.weight, model.layers.19.self_attn.q_proj.lora_B.default.weight, model.layers.19.self_attn.k_proj.lora_A.default.weight, model.layers.19.self_attn.k_proj.lora_B.default.weight, model.layers.19.self_attn.v_proj.lora_A.default.weight, model.layers.19.self_attn.v_proj.lora_B.default.weight, model.layers.19.self_attn.o_proj.lora_A.default.weight, model.layers.19.self_attn.o_proj.lora_B.default.weight, model.layers.20.self_attn.q_proj.lora_A.default.weight, model.layers.20.self_attn.q_proj.lora_B.default.weight, model.layers.20.self_attn.k_proj.lora_A.default.weight, model.layers.20.self_attn.k_proj.lora_B.default.weight, model.layers.20.self_attn.v_proj.lora_A.default.weight, model.layers.20.self_attn.v_proj.lora_B.default.weight, model.layers.20.self_attn.o_proj.lora_A.default.weight, model.layers.20.self_attn.o_proj.lora_B.default.weight, model.layers.21.self_attn.q_proj.lora_A.default.weight, model.layers.21.self_attn.q_proj.lora_B.default.weight, model.layers.21.self_attn.k_proj.lora_A.default.weight, model.layers.21.self_attn.k_proj.lora_B.default.weight, model.layers.21.self_attn.v_proj.lora_A.default.weight, model.layers.21.self_attn.v_proj.lora_B.default.weight, model.layers.21.self_attn.o_proj.lora_A.default.weight, model.layers.21.self_attn.o_proj.lora_B.default.weight, model.layers.22.self_attn.q_proj.lora_A.default.weight, model.layers.22.self_attn.q_proj.lora_B.default.weight, model.layers.22.self_attn.k_proj.lora_A.default.weight, model.layers.22.self_attn.k_proj.lora_B.default.weight, model.layers.22.self_attn.v_proj.lora_A.default.weight, model.layers.22.self_attn.v_proj.lora_B.default.weight, model.layers.22.self_attn.o_proj.lora_A.default.weight, model.layers.22.self_attn.o_proj.lora_B.default.weight, model.layers.23.self_attn.q_proj.lora_A.default.weight, model.layers.23.self_attn.q_proj.lora_B.default.weight, model.layers.23.self_attn.k_proj.lora_A.default.weight, model.layers.23.self_attn.k_proj.lora_B.default.weight, model.layers.23.self_attn.v_proj.lora_A.default.weight, model.layers.23.self_attn.v_proj.lora_B.default.weight, model.layers.23.self_attn.o_proj.lora_A.default.weight, model.layers.23.self_attn.o_proj.lora_B.default.weight, model.layers.24.self_attn.q_proj.lora_A.default.weight, model.layers.24.self_attn.q_proj.lora_B.default.weight, model.layers.24.self_attn.k_proj.lora_A.default.weight, model.layers.24.self_attn.k_proj.lora_B.default.weight, model.layers.24.self_attn.v_proj.lora_A.default.weight, model.layers.24.self_attn.v_proj.lora_B.default.weight, model.layers.24.self_attn.o_proj.lora_A.default.weight, model.layers.24.self_attn.o_proj.lora_B.default.weight, model.layers.25.self_attn.q_proj.lora_A.default.weight, model.layers.25.self_attn.q_proj.lora_B.default.weight, model.layers.25.self_attn.k_proj.lora_A.default.weight, model.layers.25.self_attn.k_proj.lora_B.default.weight, model.layers.25.self_attn.v_proj.lora_A.default.weight, model.layers.25.self_attn.v_proj.lora_B.default.weight, model.layers.25.self_attn.o_proj.lora_A.default.weight, model.layers.25.self_attn.o_proj.lora_B.default.weight, model.layers.26.self_attn.q_proj.lora_A.default.weight, model.layers.26.self_attn.q_proj.lora_B.default.weight, model.layers.26.self_attn.k_proj.lora_A.default.weight, model.layers.26.self_attn.k_proj.lora_B.default.weight, model.layers.26.self_attn.v_proj.lora_A.default.weight, model.layers.26.self_attn.v_proj.lora_B.default.weight, model.layers.26.self_attn.o_proj.lora_A.default.weight, model.layers.26.self_attn.o_proj.lora_B.default.weight, model.layers.27.self_attn.q_proj.lora_A.default.weight, model.layers.27.self_attn.q_proj.lora_B.default.weight, model.layers.27.self_attn.k_proj.lora_A.default.weight, model.layers.27.self_attn.k_proj.lora_B.default.weight, model.layers.27.self_attn.v_proj.lora_A.default.weight, model.layers.27.self_attn.v_proj.lora_B.default.weight, model.layers.27.self_attn.o_proj.lora_A.default.weight, model.layers.27.self_attn.o_proj.lora_B.default.weight, model.layers.28.self_attn.q_proj.lora_A.default.weight, model.layers.28.self_attn.q_proj.lora_B.default.weight, model.layers.28.self_attn.k_proj.lora_A.default.weight, model.layers.28.self_attn.k_proj.lora_B.default.weight, model.layers.28.self_attn.v_proj.lora_A.default.weight, model.layers.28.self_attn.v_proj.lora_B.default.weight, model.layers.28.self_attn.o_proj.lora_A.default.weight, model.layers.28.self_attn.o_proj.lora_B.default.weight, model.layers.29.self_attn.q_proj.lora_A.default.weight, model.layers.29.self_attn.q_proj.lora_B.default.weight, model.layers.29.self_attn.k_proj.lora_A.default.weight, model.layers.29.self_attn.k_proj.lora_B.default.weight, model.layers.29.self_attn.v_proj.lora_A.default.weight, model.layers.29.self_attn.v_proj.lora_B.default.weight, model.layers.29.self_attn.o_proj.lora_A.default.weight, model.layers.29.self_attn.o_proj.lora_B.default.weight\n"]},{"output_type":"stream","name":"stdout","text":["The capital of France is Paris. It is a city that has been a beacon of culture, art, and innovation for centuries. Paris is known for its iconic landmarks like the Eiffel Tower, Louvre Museum, Notre-Dame Cathedral, and the Arc de Triomphe. It is also famous for its historical sites, such as the Palace of Versailles and the Louvre Museum.\n","\n","Paris is a city that has a rich history, and its capital is a place where people from all over the world come to experience its beauty and culture.\n"]}]},{"cell_type":"markdown","source":["Ora invochiamo la conversione del modello una volta per ogni metodologia di quantizzazione."],"metadata":{"id":"i3hYKUkH61DU"}},{"cell_type":"code","source":["output_path = \"./gguf-models\"\n","\n","# Available quantization methods (from convert_hf_to_gguf.py help)\n","quantization_methods = [\n","    \"f32\",    # Full precision (32-bit float)\n","    \"f16\",    # Half precision (16-bit float)\n","    \"q8_0\",   # 8-bit quantization\n","    \"tq1_0\",  # Ternary quantization\n","    \"tq2_0\",  # Ternary quantization (alternative)\n","    \"auto\"    # Automatic selection\n","]\n","\n","# Convert with different quantization methods\n","for quant in quantization_methods:\n","    try:\n","        print(f\"\\nConverting with {quant} quantization...\")\n","        gguf_path = convert_to_gguf(model_path, output_path, quant)\n","        print(f\"Model saved to: {gguf_path}\")\n","    except Exception as e:\n","        print(f\"Failed to convert with {quant} quantization: {e}\")\n","        continue\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"auppDxPhl5gm","executionInfo":{"status":"ok","timestamp":1751641285307,"user_tz":-120,"elapsed":274266,"user":{"displayName":"Nicolò Festa","userId":"09455685607482603239"}},"outputId":"1c60e903-9587-4eb5-b1f6-ee3d3d945bd4","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Converting with f32 quantization...\n","Converting model from /root/.cache/huggingface/hub/models--monadestudio--smol-function-calling/snapshots/76ae67811058acd154412ff756dcaa3d21e398b9 to GGUF format...\n","Model directory contents:\n","- adapter_config.json\n","- adapter_model.safetensors\n","- config.json\n","- special_tokens_map.json\n","- tokenizer.json\n","- .gitattributes\n","- tokenizer_config.json\n","- generation_config.json\n","- chat_template.jinja\n","- vocab.json\n","- model.safetensors\n","- merges.txt\n","- README.md\n","Cloning llama.cpp repository...\n","Converting to GGUF format...\n","Installing llama.cpp dependencies...\n","Running conversion script...\n","Command: python3 /content/llama.cpp/convert_hf_to_gguf.py --outfile /content/gguf-models/model-f32.gguf --outtype f32 --verbose /root/.cache/huggingface/hub/models--monadestudio--smol-function-calling/snapshots/76ae67811058acd154412ff756dcaa3d21e398b9\n","Conversion script errors:\n","The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n","\n","0it [00:00, ?it/s]\n","0it [00:00, ?it/s]\n","INFO:hf-to-gguf:Loading model: 76ae67811058acd154412ff756dcaa3d21e398b9\n","INFO:hf-to-gguf:Model architecture: LlamaForCausalLM\n","INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n","INFO:hf-to-gguf:Exporting model...\n","INFO:hf-to-gguf:gguf: loading model part 'model.safetensors'\n","INFO:hf-to-gguf:token_embd.weight,           torch.float32 --> F32, shape = {576, 49152}\n","INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float32 --> F32, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float32 --> F32, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float32 --> F32, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float32 --> F32, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float32 --> F32, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float32 --> F32, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float32 --> F32, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float32 --> F32, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.float32 --> F32, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.float32 --> F32, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.float32 --> F32, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.float32 --> F32, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float32 --> F32, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.float32 --> F32, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.float32 --> F32, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.float32 --> F32, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.float32 --> F32, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.float32 --> F32, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.float32 --> F32, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.float32 --> F32, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.float32 --> F32, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.float32 --> F32, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.28.attn_k.weight,        torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.28.attn_output.weight,   torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.28.attn_q.weight,        torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.28.attn_v.weight,        torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.float32 --> F32, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.29.attn_k.weight,        torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.29.attn_output.weight,   torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.29.attn_q.weight,        torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.29.attn_v.weight,        torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float32 --> F32, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float32 --> F32, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float32 --> F32, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float32 --> F32, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float32 --> F32, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float32 --> F32, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float32 --> F32, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float32 --> F32, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float32 --> F32, shape = {576, 576}\n","INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float32 --> F32, shape = {576, 192}\n","INFO:hf-to-gguf:output_norm.weight,          torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:Set meta model\n","INFO:hf-to-gguf:Set model parameters\n","INFO:hf-to-gguf:gguf: context length = 8192\n","INFO:hf-to-gguf:gguf: embedding length = 576\n","INFO:hf-to-gguf:gguf: feed forward length = 1536\n","INFO:hf-to-gguf:gguf: head count = 9\n","INFO:hf-to-gguf:gguf: key-value head count = 3\n","INFO:hf-to-gguf:gguf: rope theta = 100000\n","INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n","INFO:hf-to-gguf:gguf: file type = 0\n","INFO:hf-to-gguf:Set model quantization version\n","INFO:hf-to-gguf:Set model tokenizer\n","DEBUG:hf-to-gguf:chktok: [3805, 8866, 1116, 3805, 197, 216, 1656, 216, 197, 11181, 472, 2367, 3914, 198, 10813, 244, 218, 365, 5472, 25, 40303, 131, 321, 231, 10813, 230, 121, 31752, 365, 30404, 649, 21658, 271, 46336, 483, 25, 4636, 246, 223, 15107, 116, 243, 10813, 116, 243, 216, 35, 216, 35, 35, 216, 35, 35, 35, 216, 35, 35, 35, 35, 216, 35, 35, 35, 35, 35, 216, 35, 35, 35, 35, 35, 35, 216, 35, 35, 35, 35, 35, 35, 35, 216, 35, 35, 35, 35, 35, 35, 35, 35, 216, 35, 30, 35, 216, 35, 950, 35, 216, 35, 2026, 35, 15822, 248, 218, 40478, 131, 40478, 237, 172, 249, 229, 40478, 233, 172, 249, 220, 40478, 240, 40478, 132, 40478, 249, 172, 249, 219, 40478, 249, 40478, 112, 40478, 131, 40478, 223, 10813, 242, 219, 9148, 19805, 235, 177, 221, 128, 32632, 21949, 36149, 115, 40994, 33, 35, 33, 36, 33, 37, 33, 18614, 119, 186, 138, 248, 216, 21771, 2031, 28733, 28050, 6643, 46438, 6485, 40610, 5470, 235, 156, 228, 12681, 29441, 6511, 9175, 39511, 7872, 7855, 11193, 1969, 1969, 3725, 1093, 1093, 5592, 950, 36689, 10095, 16693, 16693, 16693, 339, 3543, 719, 637, 100, 793, 384, 506, 665, 28, 637, 3256, 346, 2090, 47, 637, 61, 441, 2090, 339, 3060, 919, 357, 28, 637, 52, 346, 702, 634, 7188, 47, 1046, 23, 25917, 253, 23, 92, 60]\n","DEBUG:hf-to-gguf:chkhsh: 855059429035d75a914d1eda9f10a876752e281a054a7a3d421ef0533e5b6249\n","DEBUG:hf-to-gguf:tokenizer.ggml.pre: 'smollm'\n","DEBUG:hf-to-gguf:chkhsh: 855059429035d75a914d1eda9f10a876752e281a054a7a3d421ef0533e5b6249\n","2025-07-04 14:59:21.748330: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1751641161.771095    2426 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1751641161.777356    2426 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","DEBUG:tensorflow:Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n","DEBUG:h5py._conv:Creating converter from 7 to 5\n","DEBUG:h5py._conv:Creating converter from 5 to 7\n","DEBUG:h5py._conv:Creating converter from 7 to 5\n","DEBUG:h5py._conv:Creating converter from 5 to 7\n","DEBUG:2025-07-04 14:59:24,877:jax._src.path:31: etils.epath found. Using etils.epath for file I/O.\n","DEBUG:jax._src.path:etils.epath found. Using etils.epath for file I/O.\n","INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n","DEBUG:matplotlib:matplotlib data path: /usr/local/lib/python3.11/dist-packages/matplotlib/mpl-data\n","DEBUG:matplotlib:CONFIGDIR=/root/.config/matplotlib\n","DEBUG:matplotlib:interactive is False\n","DEBUG:matplotlib:platform is linux\n","DEBUG:matplotlib:CACHEDIR=/root/.cache/matplotlib\n","DEBUG:matplotlib.font_manager:Using fontManager instance from /root/.cache/matplotlib/fontlist-v390.json\n","INFO:gguf.vocab:Adding 48900 merge(s).\n","INFO:gguf.vocab:Setting special token type bos to 1\n","INFO:gguf.vocab:Setting special token type eos to 2\n","INFO:gguf.vocab:Setting special token type unk to 0\n","INFO:gguf.vocab:Setting special token type pad to 2\n","INFO:gguf.vocab:Setting chat_template to {% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\n","You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n","' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n","' + message['content'] + '<|im_end|>' + '\n","'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n","' }}{% endif %}\n","INFO:gguf.gguf_writer:Writing the following files:\n","INFO:gguf.gguf_writer:/content/gguf-models/model-f32.gguf: n_tensors = 272, total_size = 538.1M\n","\n","Writing:   0%|          | 0.00/538M [00:00<?, ?byte/s]\n","Writing:  21%|██        | 113M/538M [00:00<00:02, 152Mbyte/s]\n","Writing:  24%|██▍       | 131M/538M [00:00<00:02, 154Mbyte/s]\n","Writing:  28%|██▊       | 149M/538M [00:00<00:02, 156Mbyte/s]\n","Writing:  31%|███       | 166M/538M [00:01<00:02, 157Mbyte/s]\n","Writing:  34%|███▍      | 184M/538M [00:01<00:02, 140Mbyte/s]\n","Writing:  37%|███▋      | 198M/538M [00:01<00:02, 141Mbyte/s]\n","Writing:  40%|████      | 216M/538M [00:01<00:02, 147Mbyte/s]\n","Writing:  43%|████▎     | 234M/538M [00:01<00:01, 153Mbyte/s]\n","Writing:  47%|████▋     | 251M/538M [00:01<00:01, 150Mbyte/s]\n","Writing:  50%|████▉     | 267M/538M [00:01<00:01, 143Mbyte/s]\n","Writing:  53%|█████▎    | 283M/538M [00:01<00:01, 145Mbyte/s]\n","Writing:  55%|█████▌    | 297M/538M [00:02<00:01, 143Mbyte/s]\n","Writing:  59%|█████▊    | 315M/538M [00:02<00:01, 128Mbyte/s]\n","Writing:  61%|██████    | 329M/538M [00:02<00:01, 125Mbyte/s]\n","Writing:  64%|██████▍   | 343M/538M [00:02<00:01, 123Mbyte/s]\n","Writing:  67%|██████▋   | 361M/538M [00:02<00:01, 136Mbyte/s]\n","Writing:  70%|███████   | 379M/538M [00:02<00:01, 147Mbyte/s]\n","Writing:  73%|███████▎  | 395M/538M [00:02<00:00, 145Mbyte/s]\n","Writing:  76%|███████▌  | 410M/538M [00:02<00:00, 128Mbyte/s]\n","Writing:  80%|███████▉  | 428M/538M [00:02<00:00, 139Mbyte/s]\n","Writing:  83%|████████▎ | 446M/538M [00:03<00:00, 144Mbyte/s]\n","Writing:  86%|████████▌ | 464M/538M [00:03<00:00, 148Mbyte/s]\n","Writing:  90%|█████████ | 485M/538M [00:03<00:00, 159Mbyte/s]\n","Writing:  94%|█████████▍| 506M/538M [00:03<00:00, 168Mbyte/s]\n","Writing:  97%|█████████▋| 523M/538M [00:03<00:00, 150Mbyte/s]\n","Writing: 100%|██████████| 538M/538M [00:03<00:00, 145Mbyte/s]\n","INFO:hf-to-gguf:Model successfully exported to /content/gguf-models/model-f32.gguf\n","\n","Successfully converted model to GGUF format: /content/gguf-models/model-f32.gguf\n","Model saved to: /content/gguf-models/model-f32.gguf\n","\n","Converting with f16 quantization...\n","Converting model from /root/.cache/huggingface/hub/models--monadestudio--smol-function-calling/snapshots/76ae67811058acd154412ff756dcaa3d21e398b9 to GGUF format...\n","Model directory contents:\n","- adapter_config.json\n","- adapter_model.safetensors\n","- config.json\n","- special_tokens_map.json\n","- tokenizer.json\n","- .gitattributes\n","- tokenizer_config.json\n","- generation_config.json\n","- chat_template.jinja\n","- vocab.json\n","- model.safetensors\n","- merges.txt\n","- README.md\n","Converting to GGUF format...\n","Installing llama.cpp dependencies...\n","Running conversion script...\n","Command: python3 /content/llama.cpp/convert_hf_to_gguf.py --outfile /content/gguf-models/model-f16.gguf --outtype f16 --verbose /root/.cache/huggingface/hub/models--monadestudio--smol-function-calling/snapshots/76ae67811058acd154412ff756dcaa3d21e398b9\n","Conversion script errors:\n","INFO:hf-to-gguf:Loading model: 76ae67811058acd154412ff756dcaa3d21e398b9\n","INFO:hf-to-gguf:Model architecture: LlamaForCausalLM\n","INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n","INFO:hf-to-gguf:Exporting model...\n","INFO:hf-to-gguf:gguf: loading model part 'model.safetensors'\n","INFO:hf-to-gguf:token_embd.weight,           torch.float32 --> F16, shape = {576, 49152}\n","INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float32 --> F16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float32 --> F16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float32 --> F16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float32 --> F16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float32 --> F16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float32 --> F16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float32 --> F16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float32 --> F16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.float32 --> F16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.float32 --> F16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.float32 --> F16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.float32 --> F16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float32 --> F16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.float32 --> F16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.float32 --> F16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.float32 --> F16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.float32 --> F16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.float32 --> F16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.float32 --> F16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.float32 --> F16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.float32 --> F16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.float32 --> F16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.28.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.28.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.28.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.28.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.float32 --> F16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.29.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.29.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.29.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.29.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float32 --> F16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float32 --> F16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float32 --> F16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float32 --> F16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float32 --> F16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float32 --> F16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float32 --> F16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float32 --> F16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:output_norm.weight,          torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:Set meta model\n","INFO:hf-to-gguf:Set model parameters\n","INFO:hf-to-gguf:gguf: context length = 8192\n","INFO:hf-to-gguf:gguf: embedding length = 576\n","INFO:hf-to-gguf:gguf: feed forward length = 1536\n","INFO:hf-to-gguf:gguf: head count = 9\n","INFO:hf-to-gguf:gguf: key-value head count = 3\n","INFO:hf-to-gguf:gguf: rope theta = 100000\n","INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n","INFO:hf-to-gguf:gguf: file type = 1\n","INFO:hf-to-gguf:Set model quantization version\n","INFO:hf-to-gguf:Set model tokenizer\n","DEBUG:hf-to-gguf:chktok: [3805, 8866, 1116, 3805, 197, 216, 1656, 216, 197, 11181, 472, 2367, 3914, 198, 10813, 244, 218, 365, 5472, 25, 40303, 131, 321, 231, 10813, 230, 121, 31752, 365, 30404, 649, 21658, 271, 46336, 483, 25, 4636, 246, 223, 15107, 116, 243, 10813, 116, 243, 216, 35, 216, 35, 35, 216, 35, 35, 35, 216, 35, 35, 35, 35, 216, 35, 35, 35, 35, 35, 216, 35, 35, 35, 35, 35, 35, 216, 35, 35, 35, 35, 35, 35, 35, 216, 35, 35, 35, 35, 35, 35, 35, 35, 216, 35, 30, 35, 216, 35, 950, 35, 216, 35, 2026, 35, 15822, 248, 218, 40478, 131, 40478, 237, 172, 249, 229, 40478, 233, 172, 249, 220, 40478, 240, 40478, 132, 40478, 249, 172, 249, 219, 40478, 249, 40478, 112, 40478, 131, 40478, 223, 10813, 242, 219, 9148, 19805, 235, 177, 221, 128, 32632, 21949, 36149, 115, 40994, 33, 35, 33, 36, 33, 37, 33, 18614, 119, 186, 138, 248, 216, 21771, 2031, 28733, 28050, 6643, 46438, 6485, 40610, 5470, 235, 156, 228, 12681, 29441, 6511, 9175, 39511, 7872, 7855, 11193, 1969, 1969, 3725, 1093, 1093, 5592, 950, 36689, 10095, 16693, 16693, 16693, 339, 3543, 719, 637, 100, 793, 384, 506, 665, 28, 637, 3256, 346, 2090, 47, 637, 61, 441, 2090, 339, 3060, 919, 357, 28, 637, 52, 346, 702, 634, 7188, 47, 1046, 23, 25917, 253, 23, 92, 60]\n","DEBUG:hf-to-gguf:chkhsh: 855059429035d75a914d1eda9f10a876752e281a054a7a3d421ef0533e5b6249\n","DEBUG:hf-to-gguf:tokenizer.ggml.pre: 'smollm'\n","DEBUG:hf-to-gguf:chkhsh: 855059429035d75a914d1eda9f10a876752e281a054a7a3d421ef0533e5b6249\n","2025-07-04 14:59:49.477143: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1751641189.505839    2546 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1751641189.513145    2546 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","DEBUG:tensorflow:Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n","DEBUG:h5py._conv:Creating converter from 7 to 5\n","DEBUG:h5py._conv:Creating converter from 5 to 7\n","DEBUG:h5py._conv:Creating converter from 7 to 5\n","DEBUG:h5py._conv:Creating converter from 5 to 7\n","DEBUG:2025-07-04 14:59:52,388:jax._src.path:31: etils.epath found. Using etils.epath for file I/O.\n","DEBUG:jax._src.path:etils.epath found. Using etils.epath for file I/O.\n","INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n","DEBUG:matplotlib:matplotlib data path: /usr/local/lib/python3.11/dist-packages/matplotlib/mpl-data\n","DEBUG:matplotlib:CONFIGDIR=/root/.config/matplotlib\n","DEBUG:matplotlib:interactive is False\n","DEBUG:matplotlib:platform is linux\n","DEBUG:matplotlib:CACHEDIR=/root/.cache/matplotlib\n","DEBUG:matplotlib.font_manager:Using fontManager instance from /root/.cache/matplotlib/fontlist-v390.json\n","INFO:gguf.vocab:Adding 48900 merge(s).\n","INFO:gguf.vocab:Setting special token type bos to 1\n","INFO:gguf.vocab:Setting special token type eos to 2\n","INFO:gguf.vocab:Setting special token type unk to 0\n","INFO:gguf.vocab:Setting special token type pad to 2\n","INFO:gguf.vocab:Setting chat_template to {% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\n","You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n","' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n","' + message['content'] + '<|im_end|>' + '\n","'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n","' }}{% endif %}\n","INFO:gguf.gguf_writer:Writing the following files:\n","INFO:gguf.gguf_writer:/content/gguf-models/model-f16.gguf: n_tensors = 272, total_size = 269.1M\n","\n","Writing:   0%|          | 0.00/269M [00:00<?, ?byte/s]\n","Writing:  21%|██        | 56.6M/269M [00:00<00:02, 88.5Mbyte/s]\n","Writing:  24%|██▍       | 65.5M/269M [00:00<00:02, 84.5Mbyte/s]\n","Writing:  28%|██▊       | 74.3M/269M [00:00<00:02, 74.6Mbyte/s]\n","Writing:  30%|███       | 81.4M/269M [00:01<00:02, 70.5Mbyte/s]\n","Writing:  33%|███▎      | 88.5M/269M [00:01<00:02, 66.8Mbyte/s]\n","Writing:  36%|███▌      | 95.6M/269M [00:01<00:02, 66.6Mbyte/s]\n","Writing:  38%|███▊      | 103M/269M [00:01<00:02, 67.5Mbyte/s] \n","Writing:  41%|████▏     | 112M/269M [00:01<00:02, 72.7Mbyte/s]\n","Writing:  45%|████▌     | 122M/269M [00:01<00:01, 78.5Mbyte/s]\n","Writing:  49%|████▊     | 131M/269M [00:01<00:01, 80.6Mbyte/s]\n","Writing:  52%|█████▏    | 140M/269M [00:01<00:01, 83.1Mbyte/s]\n","Writing:  56%|█████▌    | 150M/269M [00:01<00:01, 86.5Mbyte/s]\n","Writing:  60%|█████▉    | 161M/269M [00:02<00:01, 88.2Mbyte/s]\n","Writing:  64%|██████▍   | 172M/269M [00:02<00:01, 90.7Mbyte/s]\n","Writing:  68%|██████▊   | 182M/269M [00:02<00:01, 79.1Mbyte/s]\n","Writing:  71%|███████   | 191M/269M [00:02<00:01, 71.2Mbyte/s]\n","Writing:  74%|███████▍  | 200M/269M [00:02<00:00, 74.1Mbyte/s]\n","Writing:  78%|███████▊  | 209M/269M [00:02<00:00, 72.7Mbyte/s]\n","Writing:  81%|████████  | 219M/269M [00:02<00:00, 78.2Mbyte/s]\n","Writing:  86%|████████▌ | 232M/269M [00:02<00:00, 91.6Mbyte/s]\n","Writing:  91%|█████████ | 244M/269M [00:03<00:00, 98.8Mbyte/s]\n","Writing:  95%|█████████▌| 257M/269M [00:03<00:00, 102Mbyte/s] \n","Writing:  99%|█████████▉| 267M/269M [00:03<00:00, 94.6Mbyte/s]\n","Writing: 100%|██████████| 269M/269M [00:03<00:00, 80.1Mbyte/s]\n","INFO:hf-to-gguf:Model successfully exported to /content/gguf-models/model-f16.gguf\n","\n","Successfully converted model to GGUF format: /content/gguf-models/model-f16.gguf\n","Model saved to: /content/gguf-models/model-f16.gguf\n","\n","Converting with q8_0 quantization...\n","Converting model from /root/.cache/huggingface/hub/models--monadestudio--smol-function-calling/snapshots/76ae67811058acd154412ff756dcaa3d21e398b9 to GGUF format...\n","Model directory contents:\n","- adapter_config.json\n","- adapter_model.safetensors\n","- config.json\n","- special_tokens_map.json\n","- tokenizer.json\n","- .gitattributes\n","- tokenizer_config.json\n","- generation_config.json\n","- chat_template.jinja\n","- vocab.json\n","- model.safetensors\n","- merges.txt\n","- README.md\n","Converting to GGUF format...\n","Installing llama.cpp dependencies...\n","Running conversion script...\n","Command: python3 /content/llama.cpp/convert_hf_to_gguf.py --outfile /content/gguf-models/model-q8_0.gguf --outtype q8_0 --verbose /root/.cache/huggingface/hub/models--monadestudio--smol-function-calling/snapshots/76ae67811058acd154412ff756dcaa3d21e398b9\n","Conversion script errors:\n","INFO:hf-to-gguf:Loading model: 76ae67811058acd154412ff756dcaa3d21e398b9\n","INFO:hf-to-gguf:Model architecture: LlamaForCausalLM\n","INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n","INFO:hf-to-gguf:Exporting model...\n","INFO:hf-to-gguf:gguf: loading model part 'model.safetensors'\n","INFO:hf-to-gguf:token_embd.weight,           torch.float32 --> Q8_0, shape = {576, 49152}\n","INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float32 --> Q8_0, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float32 --> Q8_0, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float32 --> Q8_0, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float32 --> Q8_0, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float32 --> Q8_0, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float32 --> Q8_0, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float32 --> Q8_0, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float32 --> Q8_0, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.float32 --> Q8_0, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.float32 --> Q8_0, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.float32 --> Q8_0, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.float32 --> Q8_0, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float32 --> Q8_0, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.float32 --> Q8_0, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.float32 --> Q8_0, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.float32 --> Q8_0, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.float32 --> Q8_0, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.float32 --> Q8_0, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.float32 --> Q8_0, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.float32 --> Q8_0, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.float32 --> Q8_0, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.float32 --> Q8_0, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.28.attn_k.weight,        torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.28.attn_output.weight,   torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.28.attn_q.weight,        torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.28.attn_v.weight,        torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.float32 --> Q8_0, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.29.attn_k.weight,        torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.29.attn_output.weight,   torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.29.attn_q.weight,        torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.29.attn_v.weight,        torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float32 --> Q8_0, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float32 --> Q8_0, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float32 --> Q8_0, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float32 --> Q8_0, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float32 --> Q8_0, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float32 --> Q8_0, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float32 --> Q8_0, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float32 --> Q8_0, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float32 --> Q8_0, shape = {576, 576}\n","INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float32 --> Q8_0, shape = {576, 192}\n","INFO:hf-to-gguf:output_norm.weight,          torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:Set meta model\n","INFO:hf-to-gguf:Set model parameters\n","INFO:hf-to-gguf:gguf: context length = 8192\n","INFO:hf-to-gguf:gguf: embedding length = 576\n","INFO:hf-to-gguf:gguf: feed forward length = 1536\n","INFO:hf-to-gguf:gguf: head count = 9\n","INFO:hf-to-gguf:gguf: key-value head count = 3\n","INFO:hf-to-gguf:gguf: rope theta = 100000\n","INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n","INFO:hf-to-gguf:gguf: file type = 7\n","INFO:hf-to-gguf:Set model quantization version\n","INFO:hf-to-gguf:Set model tokenizer\n","DEBUG:hf-to-gguf:chktok: [3805, 8866, 1116, 3805, 197, 216, 1656, 216, 197, 11181, 472, 2367, 3914, 198, 10813, 244, 218, 365, 5472, 25, 40303, 131, 321, 231, 10813, 230, 121, 31752, 365, 30404, 649, 21658, 271, 46336, 483, 25, 4636, 246, 223, 15107, 116, 243, 10813, 116, 243, 216, 35, 216, 35, 35, 216, 35, 35, 35, 216, 35, 35, 35, 35, 216, 35, 35, 35, 35, 35, 216, 35, 35, 35, 35, 35, 35, 216, 35, 35, 35, 35, 35, 35, 35, 216, 35, 35, 35, 35, 35, 35, 35, 35, 216, 35, 30, 35, 216, 35, 950, 35, 216, 35, 2026, 35, 15822, 248, 218, 40478, 131, 40478, 237, 172, 249, 229, 40478, 233, 172, 249, 220, 40478, 240, 40478, 132, 40478, 249, 172, 249, 219, 40478, 249, 40478, 112, 40478, 131, 40478, 223, 10813, 242, 219, 9148, 19805, 235, 177, 221, 128, 32632, 21949, 36149, 115, 40994, 33, 35, 33, 36, 33, 37, 33, 18614, 119, 186, 138, 248, 216, 21771, 2031, 28733, 28050, 6643, 46438, 6485, 40610, 5470, 235, 156, 228, 12681, 29441, 6511, 9175, 39511, 7872, 7855, 11193, 1969, 1969, 3725, 1093, 1093, 5592, 950, 36689, 10095, 16693, 16693, 16693, 339, 3543, 719, 637, 100, 793, 384, 506, 665, 28, 637, 3256, 346, 2090, 47, 637, 61, 441, 2090, 339, 3060, 919, 357, 28, 637, 52, 346, 702, 634, 7188, 47, 1046, 23, 25917, 253, 23, 92, 60]\n","DEBUG:hf-to-gguf:chkhsh: 855059429035d75a914d1eda9f10a876752e281a054a7a3d421ef0533e5b6249\n","DEBUG:hf-to-gguf:tokenizer.ggml.pre: 'smollm'\n","DEBUG:hf-to-gguf:chkhsh: 855059429035d75a914d1eda9f10a876752e281a054a7a3d421ef0533e5b6249\n","2025-07-04 15:00:10.071034: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1751641210.106711    2661 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1751641210.117440    2661 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","DEBUG:tensorflow:Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n","DEBUG:h5py._conv:Creating converter from 7 to 5\n","DEBUG:h5py._conv:Creating converter from 5 to 7\n","DEBUG:h5py._conv:Creating converter from 7 to 5\n","DEBUG:h5py._conv:Creating converter from 5 to 7\n","DEBUG:2025-07-04 15:00:12,746:jax._src.path:31: etils.epath found. Using etils.epath for file I/O.\n","DEBUG:jax._src.path:etils.epath found. Using etils.epath for file I/O.\n","INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n","DEBUG:matplotlib:matplotlib data path: /usr/local/lib/python3.11/dist-packages/matplotlib/mpl-data\n","DEBUG:matplotlib:CONFIGDIR=/root/.config/matplotlib\n","DEBUG:matplotlib:interactive is False\n","DEBUG:matplotlib:platform is linux\n","DEBUG:matplotlib:CACHEDIR=/root/.cache/matplotlib\n","DEBUG:matplotlib.font_manager:Using fontManager instance from /root/.cache/matplotlib/fontlist-v390.json\n","INFO:gguf.vocab:Adding 48900 merge(s).\n","INFO:gguf.vocab:Setting special token type bos to 1\n","INFO:gguf.vocab:Setting special token type eos to 2\n","INFO:gguf.vocab:Setting special token type unk to 0\n","INFO:gguf.vocab:Setting special token type pad to 2\n","INFO:gguf.vocab:Setting chat_template to {% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\n","You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n","' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n","' + message['content'] + '<|im_end|>' + '\n","'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n","' }}{% endif %}\n","INFO:gguf.gguf_writer:Writing the following files:\n","INFO:gguf.gguf_writer:/content/gguf-models/model-q8_0.gguf: n_tensors = 272, total_size = 143.0M\n","\n","Writing:   0%|          | 0.00/143M [00:00<?, ?byte/s]\n","Writing:  21%|██        | 30.1M/143M [00:00<00:02, 56.4Mbyte/s]\n","Writing:  25%|██▍       | 35.7M/143M [00:00<00:01, 56.0Mbyte/s]\n","Writing:  29%|██▉       | 41.4M/143M [00:00<00:01, 53.5Mbyte/s]\n","Writing:  33%|███▎      | 47.0M/143M [00:00<00:01, 53.4Mbyte/s]\n","Writing:  37%|███▋      | 52.6M/143M [00:00<00:01, 53.7Mbyte/s]\n","Writing:  41%|████      | 58.3M/143M [00:01<00:01, 53.2Mbyte/s]\n","Writing:  45%|████▍     | 63.8M/143M [00:01<00:01, 52.4Mbyte/s]\n","Writing:  49%|████▊     | 69.6M/143M [00:01<00:01, 48.3Mbyte/s]\n","Writing:  52%|█████▏    | 74.8M/143M [00:01<00:01, 40.0Mbyte/s]\n","Writing:  55%|█████▌    | 79.0M/143M [00:01<00:01, 38.3Mbyte/s]\n","Writing:  59%|█████▉    | 84.7M/143M [00:01<00:01, 40.7Mbyte/s]\n","Writing:  62%|██████▏   | 89.4M/143M [00:01<00:01, 40.8Mbyte/s]\n","Writing:  66%|██████▋   | 95.0M/143M [00:01<00:01, 43.7Mbyte/s]\n","Writing:  70%|███████   | 101M/143M [00:02<00:00, 44.8Mbyte/s] \n","Writing:  74%|███████▍  | 106M/143M [00:02<00:00, 46.3Mbyte/s]\n","Writing:  78%|███████▊  | 112M/143M [00:02<00:00, 48.0Mbyte/s]\n","Writing:  83%|████████▎ | 119M/143M [00:02<00:00, 50.8Mbyte/s]\n","Writing:  87%|████████▋ | 124M/143M [00:02<00:00, 51.0Mbyte/s]\n","Writing:  91%|█████████ | 130M/143M [00:02<00:00, 52.7Mbyte/s]\n","Writing:  95%|█████████▍| 135M/143M [00:02<00:00, 52.5Mbyte/s]\n","Writing:  99%|█████████▊| 141M/143M [00:02<00:00, 50.7Mbyte/s]\n","Writing: 100%|██████████| 143M/143M [00:02<00:00, 48.7Mbyte/s]\n","INFO:hf-to-gguf:Model successfully exported to /content/gguf-models/model-q8_0.gguf\n","\n","Successfully converted model to GGUF format: /content/gguf-models/model-q8_0.gguf\n","Model saved to: /content/gguf-models/model-q8_0.gguf\n","\n","Converting with tq1_0 quantization...\n","Converting model from /root/.cache/huggingface/hub/models--monadestudio--smol-function-calling/snapshots/76ae67811058acd154412ff756dcaa3d21e398b9 to GGUF format...\n","Model directory contents:\n","- adapter_config.json\n","- adapter_model.safetensors\n","- config.json\n","- special_tokens_map.json\n","- tokenizer.json\n","- .gitattributes\n","- tokenizer_config.json\n","- generation_config.json\n","- chat_template.jinja\n","- vocab.json\n","- model.safetensors\n","- merges.txt\n","- README.md\n","Converting to GGUF format...\n","Installing llama.cpp dependencies...\n","Running conversion script...\n","Command: python3 /content/llama.cpp/convert_hf_to_gguf.py --outfile /content/gguf-models/model-tq1_0.gguf --outtype tq1_0 --verbose /root/.cache/huggingface/hub/models--monadestudio--smol-function-calling/snapshots/76ae67811058acd154412ff756dcaa3d21e398b9\n","Conversion script errors:\n","INFO:hf-to-gguf:Loading model: 76ae67811058acd154412ff756dcaa3d21e398b9\n","INFO:hf-to-gguf:Model architecture: LlamaForCausalLM\n","INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n","INFO:hf-to-gguf:Exporting model...\n","INFO:hf-to-gguf:gguf: loading model part 'model.safetensors'\n","INFO:hf-to-gguf:token_embd.weight,           torch.float32 --> F16, shape = {576, 49152}\n","INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float32 --> TQ1_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float32 --> TQ1_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float32 --> TQ1_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float32 --> TQ1_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float32 --> TQ1_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float32 --> TQ1_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float32 --> TQ1_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float32 --> TQ1_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.float32 --> TQ1_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.float32 --> TQ1_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.float32 --> TQ1_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.float32 --> TQ1_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float32 --> TQ1_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.float32 --> TQ1_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.float32 --> TQ1_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.float32 --> TQ1_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.float32 --> TQ1_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.float32 --> TQ1_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.float32 --> TQ1_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.float32 --> TQ1_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.float32 --> TQ1_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.float32 --> TQ1_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.28.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.28.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.28.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.28.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.float32 --> TQ1_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.29.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.29.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.29.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.29.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float32 --> TQ1_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float32 --> TQ1_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float32 --> TQ1_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float32 --> TQ1_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float32 --> TQ1_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float32 --> TQ1_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float32 --> TQ1_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ1_0, falling back to F16\n","INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:output_norm.weight,          torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:Set meta model\n","INFO:hf-to-gguf:Set model parameters\n","INFO:hf-to-gguf:gguf: context length = 8192\n","INFO:hf-to-gguf:gguf: embedding length = 576\n","INFO:hf-to-gguf:gguf: feed forward length = 1536\n","INFO:hf-to-gguf:gguf: head count = 9\n","INFO:hf-to-gguf:gguf: key-value head count = 3\n","INFO:hf-to-gguf:gguf: rope theta = 100000\n","INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n","INFO:hf-to-gguf:gguf: file type = 36\n","INFO:hf-to-gguf:Set model quantization version\n","INFO:hf-to-gguf:Set model tokenizer\n","DEBUG:hf-to-gguf:chktok: [3805, 8866, 1116, 3805, 197, 216, 1656, 216, 197, 11181, 472, 2367, 3914, 198, 10813, 244, 218, 365, 5472, 25, 40303, 131, 321, 231, 10813, 230, 121, 31752, 365, 30404, 649, 21658, 271, 46336, 483, 25, 4636, 246, 223, 15107, 116, 243, 10813, 116, 243, 216, 35, 216, 35, 35, 216, 35, 35, 35, 216, 35, 35, 35, 35, 216, 35, 35, 35, 35, 35, 216, 35, 35, 35, 35, 35, 35, 216, 35, 35, 35, 35, 35, 35, 35, 216, 35, 35, 35, 35, 35, 35, 35, 35, 216, 35, 30, 35, 216, 35, 950, 35, 216, 35, 2026, 35, 15822, 248, 218, 40478, 131, 40478, 237, 172, 249, 229, 40478, 233, 172, 249, 220, 40478, 240, 40478, 132, 40478, 249, 172, 249, 219, 40478, 249, 40478, 112, 40478, 131, 40478, 223, 10813, 242, 219, 9148, 19805, 235, 177, 221, 128, 32632, 21949, 36149, 115, 40994, 33, 35, 33, 36, 33, 37, 33, 18614, 119, 186, 138, 248, 216, 21771, 2031, 28733, 28050, 6643, 46438, 6485, 40610, 5470, 235, 156, 228, 12681, 29441, 6511, 9175, 39511, 7872, 7855, 11193, 1969, 1969, 3725, 1093, 1093, 5592, 950, 36689, 10095, 16693, 16693, 16693, 339, 3543, 719, 637, 100, 793, 384, 506, 665, 28, 637, 3256, 346, 2090, 47, 637, 61, 441, 2090, 339, 3060, 919, 357, 28, 637, 52, 346, 702, 634, 7188, 47, 1046, 23, 25917, 253, 23, 92, 60]\n","DEBUG:hf-to-gguf:chkhsh: 855059429035d75a914d1eda9f10a876752e281a054a7a3d421ef0533e5b6249\n","DEBUG:hf-to-gguf:tokenizer.ggml.pre: 'smollm'\n","DEBUG:hf-to-gguf:chkhsh: 855059429035d75a914d1eda9f10a876752e281a054a7a3d421ef0533e5b6249\n","2025-07-04 15:00:34.007317: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1751641234.027961    2772 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1751641234.033776    2772 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","DEBUG:tensorflow:Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n","DEBUG:h5py._conv:Creating converter from 7 to 5\n","DEBUG:h5py._conv:Creating converter from 5 to 7\n","DEBUG:h5py._conv:Creating converter from 7 to 5\n","DEBUG:h5py._conv:Creating converter from 5 to 7\n","DEBUG:2025-07-04 15:00:36,526:jax._src.path:31: etils.epath found. Using etils.epath for file I/O.\n","DEBUG:jax._src.path:etils.epath found. Using etils.epath for file I/O.\n","INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n","DEBUG:matplotlib:matplotlib data path: /usr/local/lib/python3.11/dist-packages/matplotlib/mpl-data\n","DEBUG:matplotlib:CONFIGDIR=/root/.config/matplotlib\n","DEBUG:matplotlib:interactive is False\n","DEBUG:matplotlib:platform is linux\n","DEBUG:matplotlib:CACHEDIR=/root/.cache/matplotlib\n","DEBUG:matplotlib.font_manager:Using fontManager instance from /root/.cache/matplotlib/fontlist-v390.json\n","INFO:gguf.vocab:Adding 48900 merge(s).\n","INFO:gguf.vocab:Setting special token type bos to 1\n","INFO:gguf.vocab:Setting special token type eos to 2\n","INFO:gguf.vocab:Setting special token type unk to 0\n","INFO:gguf.vocab:Setting special token type pad to 2\n","INFO:gguf.vocab:Setting chat_template to {% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\n","You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n","' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n","' + message['content'] + '<|im_end|>' + '\n","'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n","' }}{% endif %}\n","INFO:gguf.gguf_writer:Writing the following files:\n","INFO:gguf.gguf_writer:/content/gguf-models/model-tq1_0.gguf: n_tensors = 272, total_size = 221.6M\n","\n","Writing:   0%|          | 0.00/222M [00:00<?, ?byte/s]\n","Writing:  26%|██▌       | 56.6M/222M [00:00<00:00, 225Mbyte/s]\n","Writing:  36%|███▋      | 80.6M/222M [00:00<00:00, 149Mbyte/s]\n","Writing:  44%|████▍     | 97.1M/222M [00:00<00:00, 141Mbyte/s]\n","Writing:  50%|█████     | 111M/222M [00:00<00:00, 138Mbyte/s] \n","Writing:  57%|█████▋    | 126M/222M [00:00<00:00, 135Mbyte/s]\n","Writing:  64%|██████▎   | 141M/222M [00:00<00:00, 131Mbyte/s]\n","Writing:  70%|██████▉   | 154M/222M [00:01<00:00, 129Mbyte/s]\n","Writing:  75%|███████▌  | 167M/222M [00:01<00:00, 115Mbyte/s]\n","Writing:  81%|████████  | 180M/222M [00:01<00:00, 118Mbyte/s]\n","Writing:  87%|████████▋ | 192M/222M [00:01<00:00, 120Mbyte/s]\n","Writing:  92%|█████████▏| 205M/222M [00:01<00:00, 121Mbyte/s]\n","Writing:  98%|█████████▊| 218M/222M [00:01<00:00, 114Mbyte/s]\n","Writing: 100%|██████████| 222M/222M [00:01<00:00, 129Mbyte/s]\n","INFO:hf-to-gguf:Model successfully exported to /content/gguf-models/model-tq1_0.gguf\n","\n","Successfully converted model to GGUF format: /content/gguf-models/model-tq1_0.gguf\n","Model saved to: /content/gguf-models/model-tq1_0.gguf\n","\n","Converting with tq2_0 quantization...\n","Converting model from /root/.cache/huggingface/hub/models--monadestudio--smol-function-calling/snapshots/76ae67811058acd154412ff756dcaa3d21e398b9 to GGUF format...\n","Model directory contents:\n","- adapter_config.json\n","- adapter_model.safetensors\n","- config.json\n","- special_tokens_map.json\n","- tokenizer.json\n","- .gitattributes\n","- tokenizer_config.json\n","- generation_config.json\n","- chat_template.jinja\n","- vocab.json\n","- model.safetensors\n","- merges.txt\n","- README.md\n","Converting to GGUF format...\n","Installing llama.cpp dependencies...\n","Running conversion script...\n","Command: python3 /content/llama.cpp/convert_hf_to_gguf.py --outfile /content/gguf-models/model-tq2_0.gguf --outtype tq2_0 --verbose /root/.cache/huggingface/hub/models--monadestudio--smol-function-calling/snapshots/76ae67811058acd154412ff756dcaa3d21e398b9\n","Conversion script errors:\n","INFO:hf-to-gguf:Loading model: 76ae67811058acd154412ff756dcaa3d21e398b9\n","INFO:hf-to-gguf:Model architecture: LlamaForCausalLM\n","INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n","INFO:hf-to-gguf:Exporting model...\n","INFO:hf-to-gguf:gguf: loading model part 'model.safetensors'\n","INFO:hf-to-gguf:token_embd.weight,           torch.float32 --> F16, shape = {576, 49152}\n","INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float32 --> TQ2_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float32 --> TQ2_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float32 --> TQ2_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float32 --> TQ2_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float32 --> TQ2_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float32 --> TQ2_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float32 --> TQ2_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float32 --> TQ2_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.float32 --> TQ2_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.float32 --> TQ2_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.float32 --> TQ2_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.float32 --> TQ2_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float32 --> TQ2_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.float32 --> TQ2_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.float32 --> TQ2_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.float32 --> TQ2_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.float32 --> TQ2_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.float32 --> TQ2_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.float32 --> TQ2_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.float32 --> TQ2_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.float32 --> TQ2_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.float32 --> TQ2_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.28.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.28.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.28.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.28.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.float32 --> TQ2_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.29.attn_k.weight,        torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.29.attn_output.weight,   torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.29.attn_q.weight,        torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.29.attn_v.weight,        torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float32 --> TQ2_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float32 --> TQ2_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float32 --> TQ2_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float32 --> TQ2_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float32 --> TQ2_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float32 --> TQ2_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float32 --> TQ2_0, shape = {1536, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float32 --> F16, shape = {576, 1536}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (1536, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float32 --> F16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float32 --> F16, shape = {576, 192}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (576, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float32 --> F16, shape = {576, 576}\n","WARNING:hf-to-gguf:Can't quantize tensor with shape (192, 576) to TQ2_0, falling back to F16\n","INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float32 --> F16, shape = {576, 192}\n","INFO:hf-to-gguf:output_norm.weight,          torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:Set meta model\n","INFO:hf-to-gguf:Set model parameters\n","INFO:hf-to-gguf:gguf: context length = 8192\n","INFO:hf-to-gguf:gguf: embedding length = 576\n","INFO:hf-to-gguf:gguf: feed forward length = 1536\n","INFO:hf-to-gguf:gguf: head count = 9\n","INFO:hf-to-gguf:gguf: key-value head count = 3\n","INFO:hf-to-gguf:gguf: rope theta = 100000\n","INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n","INFO:hf-to-gguf:gguf: file type = 37\n","INFO:hf-to-gguf:Set model quantization version\n","INFO:hf-to-gguf:Set model tokenizer\n","DEBUG:hf-to-gguf:chktok: [3805, 8866, 1116, 3805, 197, 216, 1656, 216, 197, 11181, 472, 2367, 3914, 198, 10813, 244, 218, 365, 5472, 25, 40303, 131, 321, 231, 10813, 230, 121, 31752, 365, 30404, 649, 21658, 271, 46336, 483, 25, 4636, 246, 223, 15107, 116, 243, 10813, 116, 243, 216, 35, 216, 35, 35, 216, 35, 35, 35, 216, 35, 35, 35, 35, 216, 35, 35, 35, 35, 35, 216, 35, 35, 35, 35, 35, 35, 216, 35, 35, 35, 35, 35, 35, 35, 216, 35, 35, 35, 35, 35, 35, 35, 35, 216, 35, 30, 35, 216, 35, 950, 35, 216, 35, 2026, 35, 15822, 248, 218, 40478, 131, 40478, 237, 172, 249, 229, 40478, 233, 172, 249, 220, 40478, 240, 40478, 132, 40478, 249, 172, 249, 219, 40478, 249, 40478, 112, 40478, 131, 40478, 223, 10813, 242, 219, 9148, 19805, 235, 177, 221, 128, 32632, 21949, 36149, 115, 40994, 33, 35, 33, 36, 33, 37, 33, 18614, 119, 186, 138, 248, 216, 21771, 2031, 28733, 28050, 6643, 46438, 6485, 40610, 5470, 235, 156, 228, 12681, 29441, 6511, 9175, 39511, 7872, 7855, 11193, 1969, 1969, 3725, 1093, 1093, 5592, 950, 36689, 10095, 16693, 16693, 16693, 339, 3543, 719, 637, 100, 793, 384, 506, 665, 28, 637, 3256, 346, 2090, 47, 637, 61, 441, 2090, 339, 3060, 919, 357, 28, 637, 52, 346, 702, 634, 7188, 47, 1046, 23, 25917, 253, 23, 92, 60]\n","DEBUG:hf-to-gguf:chkhsh: 855059429035d75a914d1eda9f10a876752e281a054a7a3d421ef0533e5b6249\n","DEBUG:hf-to-gguf:tokenizer.ggml.pre: 'smollm'\n","DEBUG:hf-to-gguf:chkhsh: 855059429035d75a914d1eda9f10a876752e281a054a7a3d421ef0533e5b6249\n","2025-07-04 15:00:52.035238: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1751641252.055494    2855 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1751641252.061245    2855 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","DEBUG:tensorflow:Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n","DEBUG:h5py._conv:Creating converter from 7 to 5\n","DEBUG:h5py._conv:Creating converter from 5 to 7\n","DEBUG:h5py._conv:Creating converter from 7 to 5\n","DEBUG:h5py._conv:Creating converter from 5 to 7\n","DEBUG:2025-07-04 15:00:54,064:jax._src.path:31: etils.epath found. Using etils.epath for file I/O.\n","DEBUG:jax._src.path:etils.epath found. Using etils.epath for file I/O.\n","INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n","DEBUG:matplotlib:matplotlib data path: /usr/local/lib/python3.11/dist-packages/matplotlib/mpl-data\n","DEBUG:matplotlib:CONFIGDIR=/root/.config/matplotlib\n","DEBUG:matplotlib:interactive is False\n","DEBUG:matplotlib:platform is linux\n","DEBUG:matplotlib:CACHEDIR=/root/.cache/matplotlib\n","DEBUG:matplotlib.font_manager:Using fontManager instance from /root/.cache/matplotlib/fontlist-v390.json\n","INFO:gguf.vocab:Adding 48900 merge(s).\n","INFO:gguf.vocab:Setting special token type bos to 1\n","INFO:gguf.vocab:Setting special token type eos to 2\n","INFO:gguf.vocab:Setting special token type unk to 0\n","INFO:gguf.vocab:Setting special token type pad to 2\n","INFO:gguf.vocab:Setting chat_template to {% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\n","You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n","' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n","' + message['content'] + '<|im_end|>' + '\n","'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n","' }}{% endif %}\n","INFO:gguf.gguf_writer:Writing the following files:\n","INFO:gguf.gguf_writer:/content/gguf-models/model-tq2_0.gguf: n_tensors = 272, total_size = 222.9M\n","\n","Writing:   0%|          | 0.00/223M [00:00<?, ?byte/s]\n","Writing:  25%|██▌       | 56.6M/223M [00:00<00:00, 187Mbyte/s]\n","Writing:  35%|███▍      | 77.0M/223M [00:00<00:00, 154Mbyte/s]\n","Writing:  41%|████      | 91.9M/223M [00:00<00:01, 118Mbyte/s]\n","Writing:  47%|████▋     | 105M/223M [00:00<00:01, 115Mbyte/s] \n","Writing:  52%|█████▏    | 116M/223M [00:00<00:00, 110Mbyte/s]\n","Writing:  57%|█████▋    | 127M/223M [00:01<00:00, 110Mbyte/s]\n","Writing:  62%|██████▏   | 139M/223M [00:01<00:00, 110Mbyte/s]\n","Writing:  67%|██████▋   | 150M/223M [00:01<00:00, 107Mbyte/s]\n","Writing:  73%|███████▎  | 162M/223M [00:01<00:00, 108Mbyte/s]\n","Writing:  79%|███████▊  | 175M/223M [00:01<00:00, 110Mbyte/s]\n","Writing:  83%|████████▎ | 186M/223M [00:01<00:00, 95.5Mbyte/s]\n","Writing:  88%|████████▊ | 197M/223M [00:01<00:00, 88.1Mbyte/s]\n","Writing:  93%|█████████▎| 206M/223M [00:01<00:00, 89.2Mbyte/s]\n","Writing:  98%|█████████▊| 218M/223M [00:01<00:00, 92.3Mbyte/s]\n","Writing: 100%|██████████| 223M/223M [00:02<00:00, 109Mbyte/s] \n","INFO:hf-to-gguf:Model successfully exported to /content/gguf-models/model-tq2_0.gguf\n","\n","Successfully converted model to GGUF format: /content/gguf-models/model-tq2_0.gguf\n","Model saved to: /content/gguf-models/model-tq2_0.gguf\n","\n","Converting with auto quantization...\n","Converting model from /root/.cache/huggingface/hub/models--monadestudio--smol-function-calling/snapshots/76ae67811058acd154412ff756dcaa3d21e398b9 to GGUF format...\n","Model directory contents:\n","- adapter_config.json\n","- adapter_model.safetensors\n","- config.json\n","- special_tokens_map.json\n","- tokenizer.json\n","- .gitattributes\n","- tokenizer_config.json\n","- generation_config.json\n","- chat_template.jinja\n","- vocab.json\n","- model.safetensors\n","- merges.txt\n","- README.md\n","Converting to GGUF format...\n","Installing llama.cpp dependencies...\n","Running conversion script...\n","Command: python3 /content/llama.cpp/convert_hf_to_gguf.py --outfile /content/gguf-models/model-auto.gguf --outtype auto --verbose /root/.cache/huggingface/hub/models--monadestudio--smol-function-calling/snapshots/76ae67811058acd154412ff756dcaa3d21e398b9\n","Conversion script errors:\n","INFO:hf-to-gguf:Loading model: 76ae67811058acd154412ff756dcaa3d21e398b9\n","INFO:hf-to-gguf:Model architecture: LlamaForCausalLM\n","INFO:hf-to-gguf:gguf: loading model part 'model.safetensors'\n","INFO:hf-to-gguf:choosing --outtype bf16 from first tensor type (torch.float32)\n","INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n","INFO:hf-to-gguf:Exporting model...\n","INFO:hf-to-gguf:gguf: loading model part 'model.safetensors'\n","INFO:hf-to-gguf:token_embd.weight,           torch.float32 --> BF16, shape = {576, 49152}\n","INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float32 --> BF16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float32 --> BF16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float32 --> BF16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float32 --> BF16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float32 --> BF16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float32 --> BF16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float32 --> BF16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float32 --> BF16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.float32 --> BF16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.float32 --> BF16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.float32 --> BF16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.float32 --> BF16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float32 --> BF16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.float32 --> BF16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.float32 --> BF16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.float32 --> BF16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.float32 --> BF16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.float32 --> BF16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.float32 --> BF16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.float32 --> BF16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.float32 --> BF16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.float32 --> BF16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.28.attn_k.weight,        torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.28.attn_output.weight,   torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.28.attn_q.weight,        torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.28.attn_v.weight,        torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.float32 --> BF16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.29.attn_k.weight,        torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.29.attn_output.weight,   torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.29.attn_q.weight,        torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.29.attn_v.weight,        torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float32 --> BF16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float32 --> BF16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float32 --> BF16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float32 --> BF16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float32 --> BF16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float32 --> BF16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float32 --> BF16, shape = {1536, 576}\n","INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float32 --> BF16, shape = {576, 1536}\n","INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float32 --> BF16, shape = {576, 576}\n","INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float32 --> BF16, shape = {576, 192}\n","INFO:hf-to-gguf:output_norm.weight,          torch.float32 --> F32, shape = {576}\n","INFO:hf-to-gguf:Set meta model\n","INFO:hf-to-gguf:Set model parameters\n","INFO:hf-to-gguf:gguf: context length = 8192\n","INFO:hf-to-gguf:gguf: embedding length = 576\n","INFO:hf-to-gguf:gguf: feed forward length = 1536\n","INFO:hf-to-gguf:gguf: head count = 9\n","INFO:hf-to-gguf:gguf: key-value head count = 3\n","INFO:hf-to-gguf:gguf: rope theta = 100000\n","INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n","INFO:hf-to-gguf:gguf: file type = 32\n","INFO:hf-to-gguf:Set model quantization version\n","INFO:hf-to-gguf:Set model tokenizer\n","DEBUG:hf-to-gguf:chktok: [3805, 8866, 1116, 3805, 197, 216, 1656, 216, 197, 11181, 472, 2367, 3914, 198, 10813, 244, 218, 365, 5472, 25, 40303, 131, 321, 231, 10813, 230, 121, 31752, 365, 30404, 649, 21658, 271, 46336, 483, 25, 4636, 246, 223, 15107, 116, 243, 10813, 116, 243, 216, 35, 216, 35, 35, 216, 35, 35, 35, 216, 35, 35, 35, 35, 216, 35, 35, 35, 35, 35, 216, 35, 35, 35, 35, 35, 35, 216, 35, 35, 35, 35, 35, 35, 35, 216, 35, 35, 35, 35, 35, 35, 35, 35, 216, 35, 30, 35, 216, 35, 950, 35, 216, 35, 2026, 35, 15822, 248, 218, 40478, 131, 40478, 237, 172, 249, 229, 40478, 233, 172, 249, 220, 40478, 240, 40478, 132, 40478, 249, 172, 249, 219, 40478, 249, 40478, 112, 40478, 131, 40478, 223, 10813, 242, 219, 9148, 19805, 235, 177, 221, 128, 32632, 21949, 36149, 115, 40994, 33, 35, 33, 36, 33, 37, 33, 18614, 119, 186, 138, 248, 216, 21771, 2031, 28733, 28050, 6643, 46438, 6485, 40610, 5470, 235, 156, 228, 12681, 29441, 6511, 9175, 39511, 7872, 7855, 11193, 1969, 1969, 3725, 1093, 1093, 5592, 950, 36689, 10095, 16693, 16693, 16693, 339, 3543, 719, 637, 100, 793, 384, 506, 665, 28, 637, 3256, 346, 2090, 47, 637, 61, 441, 2090, 339, 3060, 919, 357, 28, 637, 52, 346, 702, 634, 7188, 47, 1046, 23, 25917, 253, 23, 92, 60]\n","DEBUG:hf-to-gguf:chkhsh: 855059429035d75a914d1eda9f10a876752e281a054a7a3d421ef0533e5b6249\n","DEBUG:hf-to-gguf:tokenizer.ggml.pre: 'smollm'\n","DEBUG:hf-to-gguf:chkhsh: 855059429035d75a914d1eda9f10a876752e281a054a7a3d421ef0533e5b6249\n","2025-07-04 15:01:13.745109: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1751641273.783587    2942 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1751641273.794268    2942 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","DEBUG:tensorflow:Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n","DEBUG:h5py._conv:Creating converter from 7 to 5\n","DEBUG:h5py._conv:Creating converter from 5 to 7\n","DEBUG:h5py._conv:Creating converter from 7 to 5\n","DEBUG:h5py._conv:Creating converter from 5 to 7\n","DEBUG:2025-07-04 15:01:17,892:jax._src.path:31: etils.epath found. Using etils.epath for file I/O.\n","DEBUG:jax._src.path:etils.epath found. Using etils.epath for file I/O.\n","INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n","DEBUG:matplotlib:matplotlib data path: /usr/local/lib/python3.11/dist-packages/matplotlib/mpl-data\n","DEBUG:matplotlib:CONFIGDIR=/root/.config/matplotlib\n","DEBUG:matplotlib:interactive is False\n","DEBUG:matplotlib:platform is linux\n","DEBUG:matplotlib:CACHEDIR=/root/.cache/matplotlib\n","DEBUG:matplotlib.font_manager:Using fontManager instance from /root/.cache/matplotlib/fontlist-v390.json\n","INFO:gguf.vocab:Adding 48900 merge(s).\n","INFO:gguf.vocab:Setting special token type bos to 1\n","INFO:gguf.vocab:Setting special token type eos to 2\n","INFO:gguf.vocab:Setting special token type unk to 0\n","INFO:gguf.vocab:Setting special token type pad to 2\n","INFO:gguf.vocab:Setting chat_template to {% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\n","You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n","' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n","' + message['content'] + '<|im_end|>' + '\n","'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n","' }}{% endif %}\n","INFO:gguf.gguf_writer:Writing the following files:\n","INFO:gguf.gguf_writer:/content/gguf-models/model-auto.gguf: n_tensors = 272, total_size = 269.1M\n","\n","Writing:   0%|          | 0.00/269M [00:00<?, ?byte/s]\n","Writing:  21%|██        | 56.6M/269M [00:00<00:01, 111Mbyte/s]\n","Writing:  26%|██▌       | 69.9M/269M [00:00<00:01, 113Mbyte/s]\n","Writing:  31%|███       | 83.2M/269M [00:00<00:01, 98.8Mbyte/s]\n","Writing:  35%|███▍      | 93.8M/269M [00:00<00:01, 99.2Mbyte/s]\n","Writing:  40%|████      | 108M/269M [00:01<00:01, 109Mbyte/s]  \n","Writing:  46%|████▌     | 124M/269M [00:01<00:01, 118Mbyte/s]\n","Writing:  51%|█████▏    | 138M/269M [00:01<00:01, 120Mbyte/s]\n","Writing:  57%|█████▋    | 152M/269M [00:01<00:00, 125Mbyte/s]\n","Writing:  62%|██████▏   | 166M/269M [00:01<00:00, 128Mbyte/s]\n","Writing:  67%|██████▋   | 181M/269M [00:01<00:00, 120Mbyte/s]\n","Writing:  72%|███████▏  | 193M/269M [00:01<00:00, 109Mbyte/s]\n","Writing:  76%|███████▌  | 205M/269M [00:01<00:00, 111Mbyte/s]\n","Writing:  81%|████████  | 218M/269M [00:01<00:00, 112Mbyte/s]\n","Writing:  86%|████████▌ | 230M/269M [00:02<00:00, 114Mbyte/s]\n","Writing:  90%|█████████ | 243M/269M [00:02<00:00, 116Mbyte/s]\n","Writing:  95%|█████████▍| 255M/269M [00:02<00:00, 115Mbyte/s]\n","Writing:  99%|█████████▉| 267M/269M [00:02<00:00, 117Mbyte/s]\n","Writing: 100%|██████████| 269M/269M [00:02<00:00, 112Mbyte/s]\n","INFO:hf-to-gguf:Model successfully exported to /content/gguf-models/model-auto.gguf\n","\n","Successfully converted model to GGUF format: /content/gguf-models/model-auto.gguf\n","Model saved to: /content/gguf-models/model-auto.gguf\n"]}]},{"cell_type":"markdown","source":["Fatto! Ora abbiamo un file per ogni tipologia di quantizzazione.\n","\n","Come scegliamo la quantizzazione migliore? Come dicevo, è un trade-off tra velocità e precisione. Vanno provati!\n","\n","Facciamo un test di esecuzione:"],"metadata":{"id":"IBUxlAdLruxP"}},{"cell_type":"code","source":["!pip install llama-cpp-python"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eaKkXl3XsI15","executionInfo":{"status":"ok","timestamp":1751641576862,"user_tz":-120,"elapsed":291553,"user":{"displayName":"Nicolò Festa","userId":"09455685607482603239"}},"outputId":"572ebabb-1e8a-494b-c325-e80a42fa5e68"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting llama-cpp-python\n","  Downloading llama_cpp_python-0.3.10.tar.gz (79.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.0/79.0 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (4.14.0)\n","Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (1.26.4)\n","Collecting diskcache>=5.6.1 (from llama-cpp-python)\n","  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (3.1.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n","Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n","  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hcanceled\n","\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["from llama_cpp import Llama\n","from textwrap import dedent\n","\n","def get_chat_interaction(prompt):\n","    \"\"\"Create a chat interaction with the given prompt.\"\"\"\n","    return [\n","        {\n","            \"role\": \"system\",\n","            \"content\": dedent(\"\"\"\n","You are a helpful assistant with access to the following functions. Use them if required -\n","{\n","    \"name\": \"get_weather\",\n","    \"description\": \"Get the weather at a given location\",\n","    \"parameters\": {\n","        \"type\": \"object\",\n","        \"properties\": {\n","            \"latitude\": {\n","                \"type\": \"number\",\n","                \"description\": \"The latitude of the location\"\n","            },\n","            \"longitude\": {\n","                \"type\": \"number\",\n","                \"description\": \"The longitude of the location\"\n","            }\n","        },\n","        \"required\": [\n","            \"latitude\",\n","            \"longitude\"\n","        ]\n","    }\n","}\n","{\n","    \"name\": \"get_time\",\n","    \"description\": \"Get the current time at a given location\",\n","    \"parameters\": {\n","        \"type\": \"object\",\n","        \"properties\": {\n","            \"latitude\": {\n","                \"type\": \"number\",\n","                \"description\": \"The latitude of the location\"\n","            },\n","            \"longitude\": {\n","                \"type\": \"number\",\n","                \"description\": \"The longitude of the location\"\n","            }\n","        },\n","        \"required\": [\n","            \"latitude\",\n","            \"longitude\"\n","        ]\n","    }\n","}\n","\"\"\")\n","        },\n","        {\n","            \"role\": \"user\",\n","            \"content\": prompt\n","        }\n","    ]\n","\n","def format_prompt(chat_interaction):\n","    \"\"\"Format the chat interaction into a prompt string.\"\"\"\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        \"HuggingFaceTB/SmolLM2-135M-Instruct\", padding=True, truncation=True, max_length=512\n","    )\n","    formatted_prompt = tokenizer.apply_chat_template(chat_interaction, tokenize=False)\n","\n","    return formatted_prompt\n","\n","example = \"What time is it in Tokyo (Lat: 35.6895, Long: 139.6917)?\"\n","chat = get_chat_interaction(example)\n","prompt = format_prompt(chat)\n","\n","# Carichiamo il modello gguf (usiamo la variante auto)\n","model = Llama(\n","    model_path=\"./gguf-models/model-auto.gguf\",\n","    n_ctx=512,  # Match the training context length\n","    n_batch=512,\n","    n_threads=4,  # Adjust based on your CPU\n","    n_gpu_layers=-1  # Set to -1 for all layers on GPU, 0 for CPU only\n",")\n","\n","max_tokens = 128\n","\n","response = model(\n","    prompt,\n","    max_tokens=max_tokens,\n","    temperature=0.7,\n","    top_p=0.95,\n","    stop=[\"</s>\", \"<|user|>\", \"<|system|>\", \"<|im_end|>\"],  # Stop at these tokens\n","    echo=False  # Don't include the prompt in the output\n",")\n","\n","# Extract the generated text\n","generated_text = response[\"choices\"][0][\"text\"]\n","\n","# Clean up the response\n","generated_text = generated_text.strip()\n","\n","print(generated_text)"],"metadata":{"id":"MgtCD3GwucNl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ora possiamo scaricarci il nostro modello convertito nei vari formati quantizzati."],"metadata":{"id":"xLAOuvkFpTYw"}},{"cell_type":"code","source":["import os\n","from google.colab import files\n","\n","os.system(f\"zip -r gguf_models.zip {output_path}\")\n","files.download(f\"gguf_models.zip\")"],"metadata":{"id":"OeEP2dS3XTYF"},"execution_count":null,"outputs":[]}]}